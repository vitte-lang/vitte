

module vitte.compiler.parser

import vitte.compiler.lexer as lex
import vitte.compiler.diagnostics as diag
import vitte.compiler.ast as ast

# ============================================================================
# Vitte compiler – Parser ultra complet (structure AST textuelle)
#
# Objectifs :
#   - Consommer la séquence de tokens produite par vitte.compiler.lexer.
#   - Produire une représentation de module PModule/PItem structurée au
#     niveau top-level (module/import/export/types/fn/entry points/etc.).
#   - Capturer les en-têtes (signatures) et les corps (blocs) comme texte
#     brut, avec une gestion correcte des blocs imbriqués ".end".
#   - Reporter les erreurs vers DiagnosticsSink via diag.make_parser_error.
#
# Remarques :
#   - L’AST ici est volontairement textuel pour les expressions/types.
#     Les passes ultérieures (typage, IR builder) pourront soit réutiliser
#     ce texte, soit reconstruire un AST plus riche.
#   - La structure des blocs est respectée via un comptage de profondeur
#     sur les ":" et les séquences ".end".
# ============================================================================

# ----------------------------------------------------------------------------
# AST du parser (niveau module/items)
# ----------------------------------------------------------------------------

pub enum PVisibility:
    PVisDefault
    PVisPub
.end

pub enum PItemKind:
    PItemImport
    PItemExport
    PItemStruct
    PItemUnion
    PItemEnum
    PItemTypedef
    PItemTrait
    PItemImpl
    PItemFunction
    PItemScenario
    PItemEntryPoint
    PItemMuffin
    PItemGlobalLet
    PItemGlobalConst
    PItemExpr
.end

pub struct PItem:
    let kind: PItemKind
    let visibility: PVisibility
    let name: String
    let header_text: String   # ligne / signature (sans le bloc)
    let body_text: String     # texte du bloc (éventuellement vide)
    let span: ast.Span
.end

pub struct PModule:
    let name: String
    let items: Vec<PItem>
    let span: ast.Span
.end

pub struct ParseResult:
    let module: PModule
    let diagnostics: diag.DiagnosticsSink
.end

# ----------------------------------------------------------------------------
# Erreurs de parsing
# ----------------------------------------------------------------------------

pub enum ParseErrorKind:
    ParseUnexpectedToken
    ParseExpectedToken
    ParseUnclosedBlock
.end

pub struct ParseError:
    let kind: ParseErrorKind
    let message: String
    let span: ast.Span
.end

fn make_parse_error(String msg, ast.Span span) -> ParseError:
    let err = ParseError(
        kind = ParseErrorKind::ParseUnexpectedToken,
        message = msg,
        span = span
    )
    return err
.end

# ----------------------------------------------------------------------------
# État interne du parser
# ----------------------------------------------------------------------------

pub struct Parser:
    let tokens: Vec<lex.Token>
    let index: usize
    let file_name: String
.end

fn parser_new(Vec<lex.Token> tokens, String file_name) -> Parser:
    let p = Parser(
        tokens = tokens,
        index = 0usize,
        file_name = file_name
    )
    return p
.end

fn parser_is_eof(Parser p) -> Bool:
    let len = p.tokens.len()
    return p.index >= len
.end

fn parser_peek(Parser p) -> lex.Token:
    let len = p.tokens.len()
    if len == 0usize:
        # Cas pathologique : pseudo-token EOF
        let fake_span = ast.Span(
            file = p.file_name,
            start_line = 0u32,
            start_col = 0u32,
            end_line = 0u32,
            end_col = 0u32
        )
        let t = lex.Token(
            kind = lex.TokenKind::TkEof,
            lexeme = "",
            span = fake_span
        )
        return t
    .end

    if p.index >= len:
        return p.tokens[len - 1usize]
    .end

    return p.tokens[p.index]
.end

fn parser_peek_n(Parser p, usize n) -> lex.Token:
    let len = p.tokens.len()
    let idx = p.index + n
    if len == 0usize:
        return parser_peek(p)
    .end
    if idx >= len:
        return p.tokens[len - 1usize]
    .end
    return p.tokens[idx]
.end

fn parser_advance(Parser p) -> Parser:
    let len = p.tokens.len()
    if p.index >= len:
        return p
    .end
    let np = Parser(
        tokens = p.tokens,
        index = p.index + 1usize,
        file_name = p.file_name
    )
    return np
.end

fn token_is_trivia(lex.Token tok) -> Bool:
    if tok.kind == lex.TokenKind::TkNewline:
        return true
    .end
    if tok.kind == lex.TokenKind::TkIndent:
        return true
    .end
    if tok.kind == lex.TokenKind::TkDedent:
        return true
    .end
    return false
.end

fn parser_skip_trivia(Parser p) -> Parser:
    let current = p
    while true:
        let tok = parser_peek(current)
        if token_is_trivia(tok):
            let p2 = parser_advance(current)
            let current2 = p2
        .end
        if not token_is_trivia(tok):
            return current
        .end
    .end
    return current
.end

fn parser_match(Parser p, lex.TokenKind kind) -> (Parser, Bool):
    let tok = parser_peek(p)
    if tok.kind == kind:
        let p2 = parser_advance(p)
        return (p2, true)
    .end
    return (p, false)
.end

fn parser_expect(Parser p, diag.DiagnosticsSink sink, lex.TokenKind kind, String what) -> (Parser, diag.DiagnosticsSink, lex.Token):
    let tok = parser_peek(p)
    if tok.kind == kind:
        let p2 = parser_advance(p)
        return (p2, sink, tok)
    .end

    let msg = "expected " + what
    let err_diag = diag.make_parser_error(msg, tok.span)
    sink.push(err_diag)

    # On ne consomme pas, pour rester conservateur.
    return (p, sink, tok)
.end

# ----------------------------------------------------------------------------
# Détection du marqueur ".end"
# ----------------------------------------------------------------------------

fn parser_is_end_marker(Parser p) -> Bool:
    let tok1 = parser_peek(p)
    let tok2 = parser_peek_n(p, 1usize)

    if tok1.kind == lex.TokenKind::TkDot and
       tok2.kind == lex.TokenKind::TkIdentifier and
       tok2.lexeme == "end":
        return true
    .end

    return false
.end

fn parser_consume_end_marker(Parser p) -> (Parser, ast.Span):
    let tok1 = parser_peek(p)
    let p1 = parser_advance(p)
    let tok2 = parser_peek(p1)
    let p2 = parser_advance(p1)

    # Consommer un éventuel newline juste après .end
    let tok3 = parser_peek(p2)
    if tok3.kind == lex.TokenKind::TkNewline:
        let p3 = parser_advance(p2)
        let sp = ast.Span(
            file = tok1.span.file,
            start_line = tok1.span.start_line,
            start_col = tok1.span.start_col,
            end_line = tok2.span.end_line,
            end_col = tok2.span.end_col
        )
        return (p3, sp)
    .end

    let sp2 = ast.Span(
        file = tok1.span.file,
        start_line = tok1.span.start_line,
        start_col = tok1.span.start_col,
        end_line = tok2.span.end_line,
        end_col = tok2.span.end_col
    )
    return (p2, sp2)
.end

fn is_block_header_keyword(lex.TokenKind kind) -> Bool:
    if kind == lex.TokenKind::TkKwIf:
        return true
    .end
    if kind == lex.TokenKind::TkKwElif:
        return true
    .end
    if kind == lex.TokenKind::TkKwElse:
        return true
    .end
    if kind == lex.TokenKind::TkKwWhile:
        return true
    .end
    if kind == lex.TokenKind::TkKwFor:
        return true
    .end
    if kind == lex.TokenKind::TkKwMatch:
        return true
    .end
    if kind == lex.TokenKind::TkKwStruct:
        return true
    .end
    if kind == lex.TokenKind::TkKwUnion:
        return true
    .end
    if kind == lex.TokenKind::TkKwEnum:
        return true
    .end
    if kind == lex.TokenKind::TkKwTrait:
        return true
    .end
    if kind == lex.TokenKind::TkKwImpl:
        return true
    .end
    if kind == lex.TokenKind::TkKwFn:
        return true
    .end
    if kind == lex.TokenKind::TkKwScenario:
        return true
    .end
    if kind == lex.TokenKind::TkKwProgram:
        return true
    .end
    if kind == lex.TokenKind::TkKwService:
        return true
    .end
    if kind == lex.TokenKind::TkKwKernel:
        return true
    .end
    if kind == lex.TokenKind::TkKwDriver:
        return true
    .end
    if kind == lex.TokenKind::TkKwTool:
        return true
    .end
    if kind == lex.TokenKind::TkKwPipeline:
        return true
    .end
    if kind == lex.TokenKind::TkKwMuffin:
        return true
    .end
    return false
.end

# ----------------------------------------------------------------------------
# Parsing des blocs (corps textuels avec gestion .end imbriqués)
# ----------------------------------------------------------------------------

fn parse_block_text(Parser p, diag.DiagnosticsSink sink) -> (Parser, diag.DiagnosticsSink, String, ast.Span):
    # On suppose que le prochain token est un ':' déjà non consommé.
    let tok_colon = parser_peek(p)
    let p1 = parser_advance(p)

    # On peut sauter un éventuel newline après ':'
    let p2 = parser_skip_trivia(p1)

    let body = ""
    let depth = 1u32
    let current = p2

    let first_tok = parser_peek(current)
    let body_start_line = first_tok.span.start_line
    let body_start_col = first_tok.span.start_col

    let prev_kind_opt: Option<lex.TokenKind> = Option<lex.TokenKind>::None()

    while depth > 0u32 and not parser_is_eof(current):
        if parser_is_end_marker(current):
            let end_tok1 = parser_peek(current)
            let end_tok2 = parser_peek_n(current, 1usize)

            # .end ferme le bloc courant
            depth = depth - 1u32
            if depth == 0u32:
                break
            .end

            # .end d'un bloc imbriqué : on l'ajoute au texte
            body = body + end_tok1.lexeme + " " + end_tok2.lexeme + " "
            let c1 = parser_advance(current)
            let c2 = parser_advance(c1)
            let current2 = c2
            let tmp_prev: Option<lex.TokenKind> = prev_kind_opt
            current = current2
        .end

        if not parser_is_end_marker(current):
            let tok = parser_peek(current)
            let k = tok.kind

            # Détection naïve de début de bloc imbriqué : keyword + ':'
            let next_tok = parser_peek_n(current, 1usize)
            if tok.kind == lex.TokenKind::TkColon and prev_kind_opt.is_some() and
               is_block_header_keyword(prev_kind_opt.unwrap()):
                depth = depth + 1u32
            .end

            # Ajout du lexeme au texte
            if tok.kind == lex.TokenKind::TkNewline:
                body = body + "\n"
            .end
            if tok.kind != lex.TokenKind::TkNewline and
               tok.kind != lex.TokenKind::TkIndent and
               tok.kind != lex.TokenKind::TkDedent:
                body = body + tok.lexeme + " "
            .end

            # Mise à jour du précédent token significatif
            if not token_is_trivia(tok):
                let some_k: Option<lex.TokenKind> = Option<lex.TokenKind>::Some(k)
                prev_kind_opt = some_k
            .end

            let c = parser_advance(current)
            let current3 = c
            current = current3
        .end
    .end

    if depth > 0u32 and parser_is_eof(current):
        # Bloc non fermé
        let sp_err = tok_colon.span
        let err_diag = diag.make_parser_error("unclosed block (missing .end)", sp_err)
        sink.push(err_diag)
        let sp_body = ast.Span(
            file = tok_colon.span.file,
            start_line = body_start_line,
            start_col = body_start_col,
            end_line = tok_colon.span.end_line,
            end_col = tok_colon.span.end_col
        )
        return (current, sink, body, sp_body)
    .end

    # Consommer le .end final du bloc courant
    let pair_end = parser_consume_end_marker(current)
    let p_after = pair_end.0
    let end_span = pair_end.1

    let sp = ast.Span(
        file = tok_colon.span.file,
        start_line = tok_colon.span.start_line,
        start_col = tok_colon.span.start_col,
        end_line = end_span.end_line,
        end_col = end_span.end_col
    )

    return (p_after, sink, body, sp)
.end

# ----------------------------------------------------------------------------
# Parsing des en-têtes textuels (une ligne)
# ----------------------------------------------------------------------------

fn collect_header_until(Parser p, lex.TokenKind stop_kind) -> (Parser, String, ast.Span, String):
    # Retourne (parser, header_text, span, name)
    let current = p
    let header = ""
    let name = ""

    let first = parser_peek(current)
    let start_line = first.span.start_line
    let start_col = first.span.start_col

    let seen_name = false

    while true:
        let tok = parser_peek(current)
        if tok.kind == lex.TokenKind::TkEof:
            break
        .end
        if tok.kind == stop_kind:
            break
        .end
        if tok.kind == lex.TokenKind::TkNewline:
            break
        .end

        if tok.kind == lex.TokenKind::TkIdentifier and not seen_name:
            name = tok.lexeme
            seen_name = true
        .end

        header = header + tok.lexeme + " "
        let c = parser_advance(current)
        let current2 = c
        current = current2
    .end

    let end_tok = parser_peek(current)
    let sp = ast.Span(
        file = first.span.file,
        start_line = start_line,
        start_col = start_col,
        end_line = end_tok.span.end_line,
        end_col = end_tok.span.end_col
    )

    return (current, header, sp, name)
.end

fn collect_header_to_eol(Parser p) -> (Parser, String, ast.Span, String):
    # Collecte jusqu'au prochain newline ou EOF.
    let current = p
    let header = ""
    let name = ""

    let first = parser_peek(current)
    let start_line = first.span.start_line
    let start_col = first.span.start_col

    let seen_name = false

    while true:
        let tok = parser_peek(current)
        if tok.kind == lex.TokenKind::TkEof:
            break
        .end
        if tok.kind == lex.TokenKind::TkNewline:
            break
        .end

        if tok.kind == lex.TokenKind::TkIdentifier and not seen_name:
            name = tok.lexeme
            seen_name = true
        .end

        header = header + tok.lexeme + " "
        let c = parser_advance(current)
        let current2 = c
        current = current2
    .end

    let end_tok = parser_peek(current)
    let sp = ast.Span(
        file = first.span.file,
        start_line = start_line,
        start_col = start_col,
        end_line = end_tok.span.end_line,
        end_col = end_tok.span.end_col
    )

    # Consommer un éventuel newline
    let tok_nl = parser_peek(current)
    if tok_nl.kind == lex.TokenKind::TkNewline:
        let c2 = parser_advance(current)
        let current3 = c2
        current = current3
    .end

    return (current, header, sp, name)
.end

# ----------------------------------------------------------------------------
# Parsing des items top-level
# ----------------------------------------------------------------------------

fn parse_visibility(Parser p) -> (Parser, PVisibility):
    let tok = parser_peek(p)
    if tok.kind == lex.TokenKind::TkKwPub:
        let p2 = parser_advance(p)
        return (p2, PVisibility::PVisPub)
    .end
    return (p, PVisibility::PVisDefault)
.end

fn parse_module_decl(Parser p, diag.DiagnosticsSink sink) -> (Parser, diag.DiagnosticsSink, String):
    # module core.math.point
    let p0 = parser_skip_trivia(p)
    let tok_mod = parser_peek(p0)
    let p1 = parser_advance(p0)

    let (p2, header, _sp, _name) = collect_header_to_eol(p1)
    let module_name = header

    return (p2, sink, module_name)
.end

fn parse_import_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let tok_imp = parser_peek(p0)
    let (p1, header, sp, name) = collect_header_to_eol(p0)

    let item = PItem(
        kind = PItemKind::PItemImport,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = "",
        span = sp
    )
    return (p1, sink, item)
.end

fn parse_export_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let tok_exp = parser_peek(p0)
    let (p1, header, sp, name) = collect_header_to_eol(p0)

    let item = PItem(
        kind = PItemKind::PItemExport,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = "",
        span = sp
    )
    return (p1, sink, item)
.end

fn parse_block_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis, PItemKind kind) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let tok_kw = parser_peek(p0)

    # Collecte jusqu'à ':' (début de bloc)
    let (p1, header, sp_header, name) = collect_header_until(p0, lex.TokenKind::TkColon)

    let tok_colon = parser_peek(p1)
    if tok_colon.kind != lex.TokenKind::TkColon:
        # pas de bloc : on se contente de l'en-tête
        let item0 = PItem(
            kind = kind,
            visibility = vis,
            name = name,
            header_text = header,
            body_text = "",
            span = sp_header
        )
        return (p1, sink, item0)
    .end

    let (p2, sink2, body, sp_block) = parse_block_text(p1, sink)

    let sp_full = ast.Span(
        file = sp_header.file,
        start_line = sp_header.start_line,
        start_col = sp_header.start_col,
        end_line = sp_block.end_line,
        end_col = sp_block.end_col
    )

    let item = PItem(
        kind = kind,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = body,
        span = sp_full
    )

    return (p2, sink2, item)
.end

fn parse_typedef_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let (p1, header, sp, name) = collect_header_to_eol(p0)
    let item = PItem(
        kind = PItemKind::PItemTypedef,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = "",
        span = sp
    )
    return (p1, sink, item)
.end

fn parse_global_binding_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis, Bool is_const) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let (p1, header, sp, name) = collect_header_to_eol(p0)

    let kind =
        if is_const:
            PItemKind::PItemGlobalConst
        else:
            PItemKind::PItemGlobalLet
        .end

    let item = PItem(
        kind = kind,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = "",
        span = sp
    )
    return (p1, sink, item)
.end

fn parse_expr_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let (p1, header, sp, _name) = collect_header_to_eol(p0)

    let item = PItem(
        kind = PItemKind::PItemExpr,
        visibility = vis,
        name = "",
        header_text = header,
        body_text = "",
        span = sp
    )
    return (p1, sink, item)
.end

fn parse_muffin_item(Parser p, diag.DiagnosticsSink sink, PVisibility vis) -> (Parser, diag.DiagnosticsSink, PItem):
    let p0 = parser_skip_trivia(p)
    let (p1, header, sp_header, name) = collect_header_to_eol(p0)

    # Après muffin "path" on attend un bloc de propriétés
    let tok_next = parser_peek(p1)
    if tok_next.kind != lex.TokenKind::TkColon and tok_next.kind != lex.TokenKind::TkLBrace:
        # version minimale sans bloc
        let item0 = PItem(
            kind = PItemKind::PItemMuffin,
            visibility = vis,
            name = name,
            header_text = header,
            body_text = "",
            span = sp_header
        )
        return (p1, sink, item0)
    .end

    # On réutilise parse_block_text (basé sur ': ... .end')
    let (p2, sink2, body, sp_block) = parse_block_text(p1, sink)

    let sp_full = ast.Span(
        file = sp_header.file,
        start_line = sp_header.start_line,
        start_col = sp_header.start_col,
        end_line = sp_block.end_line,
        end_col = sp_block.end_col
    )

    let item = PItem(
        kind = PItemKind::PItemMuffin,
        visibility = vis,
        name = name,
        header_text = header,
        body_text = body,
        span = sp_full
    )

    return (p2, sink2, item)
.end

fn parse_entrypoint_kind(lex.TokenKind kind) -> PItemKind:
    # Tous les mots-clés de points d'entrée sont regroupés sous PItemEntryPoint.
    return PItemKind::PItemEntryPoint
.end

fn parse_top_level_item(Parser p, diag.DiagnosticsSink sink) -> (Parser, diag.DiagnosticsSink, Option<PItem>):
    let p0 = parser_skip_trivia(p)
    let (p1, vis) = parse_visibility(p0)
    let p2 = parser_skip_trivia(p1)

    let tok = parser_peek(p2)

    if tok.kind == lex.TokenKind::TkKwModule:
        # Déclaration de module déjà gérée ailleurs
        let (p_mod, sink_mod, _name) = parse_module_decl(p2, sink)
        let none_item: Option<PItem> = Option<PItem>::None()
        return (p_mod, sink_mod, none_item)
    .end

    if tok.kind == lex.TokenKind::TkKwImport:
        let (p_imp, sink_imp, item_imp) = parse_import_item(p2, sink, vis)
        let some_item: Option<PItem> = Option<PItem>::Some(item_imp)
        return (p_imp, sink_imp, some_item)
    .end

    if tok.kind == lex.TokenKind::TkKwExport:
        let (p_exp, sink_exp, item_exp) = parse_export_item(p2, sink, vis)
        let some_item2: Option<PItem> = Option<PItem>::Some(item_exp)
        return (p_exp, sink_exp, some_item2)
    .end

    if tok.kind == lex.TokenKind::TkKwStruct:
        let (p_s, sink_s, item_s) = parse_block_item(p2, sink, vis, PItemKind::PItemStruct)
        let some_s: Option<PItem> = Option<PItem>::Some(item_s)
        return (p_s, sink_s, some_s)
    .end

    if tok.kind == lex.TokenKind::TkKwUnion:
        let (p_u, sink_u, item_u) = parse_block_item(p2, sink, vis, PItemKind::PItemUnion)
        let some_u: Option<PItem> = Option<PItem>::Some(item_u)
        return (p_u, sink_u, some_u)
    .end

    if tok.kind == lex.TokenKind::TkKwEnum:
        let (p_e, sink_e, item_e) = parse_block_item(p2, sink, vis, PItemKind::PItemEnum)
        let some_e: Option<PItem> = Option<PItem>::Some(item_e)
        return (p_e, sink_e, some_e)
    .end

    if tok.kind == lex.TokenKind::TkKwTypedef:
        let (p_t, sink_t, item_t) = parse_typedef_item(p2, sink, vis)
        let some_t: Option<PItem> = Option<PItem>::Some(item_t)
        return (p_t, sink_t, some_t)
    .end

    if tok.kind == lex.TokenKind::TkKwTrait:
        let (p_tr, sink_tr, item_tr) = parse_block_item(p2, sink, vis, PItemKind::PItemTrait)
        let some_tr: Option<PItem> = Option<PItem>::Some(item_tr)
        return (p_tr, sink_tr, some_tr)
    .end

    if tok.kind == lex.TokenKind::TkKwImpl:
        let (p_i, sink_i, item_i) = parse_block_item(p2, sink, vis, PItemKind::PItemImpl)
        let some_i: Option<PItem> = Option<PItem>::Some(item_i)
        return (p_i, sink_i, some_i)
    .end

    if tok.kind == lex.TokenKind::TkKwFn:
        let (p_f, sink_f, item_f) = parse_block_item(p2, sink, vis, PItemKind::PItemFunction)
        let some_f: Option<PItem> = Option<PItem>::Some(item_f)
        return (p_f, sink_f, some_f)
    .end

    if tok.kind == lex.TokenKind::TkKwScenario:
        let (p_sc, sink_sc, item_sc) = parse_block_item(p2, sink, vis, PItemKind::PItemScenario)
        let some_sc: Option<PItem> = Option<PItem>::Some(item_sc)
        return (p_sc, sink_sc, some_sc)
    .end

    if tok.kind == lex.TokenKind::TkKwProgram or
       tok.kind == lex.TokenKind::TkKwService or
       tok.kind == lex.TokenKind::TkKwKernel or
       tok.kind == lex.TokenKind::TkKwDriver or
       tok.kind == lex.TokenKind::TkKwTool or
       tok.kind == lex.TokenKind::TkKwPipeline:
        let kind = parse_entrypoint_kind(tok.kind)
        let (p_ep, sink_ep, item_ep) = parse_block_item(p2, sink, vis, kind)
        let some_ep: Option<PItem> = Option<PItem>::Some(item_ep)
        return (p_ep, sink_ep, some_ep)
    .end

    if tok.kind == lex.TokenKind::TkKwMuffin:
        let (p_m, sink_m, item_m) = parse_muffin_item(p2, sink, vis)
        let some_m: Option<PItem> = Option<PItem>::Some(item_m)
        return (p_m, sink_m, some_m)
    .end

    if tok.kind == lex.TokenKind::TkKwLet:
        let (p_l, sink_l, item_l) = parse_global_binding_item(p2, sink, vis, false)
        let some_l: Option<PItem> = Option<PItem>::Some(item_l)
        return (p_l, sink_l, some_l)
    .end

    if tok.kind == lex.TokenKind::TkKwConst:
        let (p_c, sink_c, item_c) = parse_global_binding_item(p2, sink, vis, true)
        let some_c: Option<PItem> = Option<PItem>::Some(item_c)
        return (p_c, sink_c, some_c)
    .end

    # Sinon : statement/expr au top-level
    let (p_x, sink_x, item_x) = parse_expr_item(p2, sink, vis)
    let some_x: Option<PItem> = Option<PItem>::Some(item_x)
    return (p_x, sink_x, some_x)
.end

# ----------------------------------------------------------------------------
# Parsing du module complet
# ----------------------------------------------------------------------------

fn make_module_span(Parser p) -> ast.Span:
    let toks = p.tokens
    let len = toks.len()
    if len == 0usize:
        let sp = ast.Span(
            file = p.file_name,
            start_line = 0u32,
            start_col = 0u32,
            end_line = 0u32,
            end_col = 0u32
        )
        return sp
    .end

    let first = toks[0usize]
    let last = toks[len - 1usize]
    let sp2 = ast.Span(
        file = first.span.file,
        start_line = first.span.start_line,
        start_col = first.span.start_col,
        end_line = last.span.end_line,
        end_col = last.span.end_col
    )
    return sp2
.end

fn parse_module_tokens(Parser p, diag.DiagnosticsSink sink) -> (Parser, diag.DiagnosticsSink, PModule):
    let current = p
    let ds = sink

    let items = Vec<PItem>::new()
    let module_name = ""

    while true:
        let current2 = parser_skip_trivia(current)
        let tok = parser_peek(current2)

        if tok.kind == lex.TokenKind::TkEof:
            let current_done = current2
            let ds_done = ds
            let mod_span = make_module_span(current_done)
            let m = PModule(
                name = module_name,
                items = items,
                span = mod_span
            )
            return (current_done, ds_done, m)
        .end

        if tok.kind == lex.TokenKind::TkKwModule and module_name == "":
            let (p_mod, ds_mod, name_mod) = parse_module_decl(current2, ds)
            module_name = name_mod
            let cur3 = p_mod
            let ds3 = ds_mod
            let tmp = 0u32
            current = cur3
            ds = ds3
        .end

        if tok.kind != lex.TokenKind::TkKwModule or module_name != "":
            let (p_item, ds_item, maybe_item) = parse_top_level_item(current2, ds)
            let cur4 = p_item
            let ds4 = ds_item
            if maybe_item.is_some():
                let it = maybe_item.unwrap()
                items.push(it)
            .end
            current = cur4
            ds = ds4
        .end
    .end

    let mod_span2 = make_module_span(current)
    let m2 = PModule(name = module_name, items = items, span = mod_span2)
    return (current, ds, m2)
.end

# ----------------------------------------------------------------------------
# API publique
# ----------------------------------------------------------------------------

pub fn parse_source(String file_name, String text, diag.DiagnosticsSink sink) -> ParseResult:
    # 1) Lexing
    let lex_res = lex.lex_source(file_name, text, sink)
    let tokens = lex_res.tokens
    let ds1 = lex_res.diagnostics

    # 2) Parsing
    let p0 = parser_new(tokens, file_name)
    let (p1, ds2, module_ast) = parse_module_tokens(p0, ds1)

    let res = ParseResult(
        module = module_ast,
        diagnostics = ds2
    )
    return res
.end

# ----------------------------------------------------------------------------
# Notes
# ----------------------------------------------------------------------------
# - Ce parser se concentre sur la structure top-level, avec blocs capturés
#   comme texte (body_text). C’est un bon compromis pour connecter rapidement
#   la pipeline :
#       source -> lexer -> parser (PModule) -> passes ultérieures.
# - Pour un AST plus riche, il suffira d’enrichir PItem et d’ajouter des
#   parseurs spécialisés pour les types, expressions et statements.
# ----------------------------------------------------------------------------