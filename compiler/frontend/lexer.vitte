module vitte.compiler.frontend.lexer

import std.collections as coll
import std.string as str

import vitte.compiler.frontend.diagnostics as diag
import vitte.compiler.span as source_span

# =============================================================================
# Vitte compiler – Lexer runtime (squelette projet complet)
#
# Objectifs :
#   - Fournir :
#       * un type TokenKind pour tous les tokens logiques,
#       * un type Token (kind + lexeme + span),
#       * un état de lexer (Lexer) avec helpers,
#       * une fonction publique lex(source, file, diags) -> Vec[Token].
#   - Servir de contrat stable pour :
#       * le parser,
#       * le driver,
#       * les tests (goldens, snapshots).
#
# Remarques :
#   - Ce fichier définit l'architecture du lexer runtime pour le compilateur.
#   - Aucun I/O ici : le texte source est déjà fourni en String.
#   - La logique de lexing (identifiants, nombres, opérateurs, etc.)
#     peut être enrichie progressivement sans changer les signatures.
# =============================================================================

# -----------------------------------------------------------------------------
# TokenKind – catégories de tokens
# -----------------------------------------------------------------------------

enum TokenKind
  # Fin de fichier ------------------------------------------------------------
  Eof

  # Identifiants et littéraux -------------------------------------------------
  Ident
  IntLiteral
  FloatLiteral
  CharLiteral
  StringLiteral

  # Mots-clés principaux (à compléter si besoin) -----------------------------
  KwModule
  KwImport
  KwExport
  KwStruct
  KwEnum
  KwUnion
  KwType
  KwFn
  KwProgram
  KwScenario
  KwPipeline
  KwEnd
  KwLet
  KwMut
  KwIf
  KwElse
  KwWhile
  KwMatch
  KwTrue
  KwFalse

  # Punctuations et opérateurs simples ---------------------------------------
  LParen         # (
  RParen         # )
  LBrace         # {
  RBrace         # }
  LBracket       # [
  RBracket       # ]
  Comma          # ,
  Colon          # :
  ColonColon     # ::
  Dot            # .
  Equal          # =
  Plus           # +
  Minus          # -
  Star           # *
  Slash          # /
  Percent        # %
  Arrow          # ->
  FatArrow       # =>
  EqualEqual     # ==
  BangEqual      # !=
  Bang           # !
  Less           # <
  LessEqual      # <=
  Greater        # >
  GreaterEqual   # >=
  AmpAmp         # &&
  PipePipe       # ||

  # Indentation / structure ---------------------------------------------------
  Newline
  Indent
  Dedent

  # Divers --------------------------------------------------------------------
  Unknown
.end

# -----------------------------------------------------------------------------
# Token – produit par le lexer
# -----------------------------------------------------------------------------

struct Token
  kind: TokenKind
  lexeme: String
  span: diag.Span
.end

fn Token.new(kind: TokenKind, lexeme: String, span: diag.Span) -> Token
  return Token(
    kind = kind,
    lexeme = lexeme,
    span = span,
  )
.end

# -----------------------------------------------------------------------------
# État du lexer
# -----------------------------------------------------------------------------

struct Lexer
  source: String
  file: String
  index: Int
  line_index: source_span.LineIndex
  tokens: coll.Vec[Token]
  diags: &mut diag.DiagnosticBag
.end

fn Lexer.new(source: String, file: String, diags: &mut diag.DiagnosticBag) -> Lexer
  let line_index = source_span.LineIndex.build(file, source)
  return Lexer(
    source = source,
    file = file,
    index = 0,
    line_index = line_index,
    tokens = coll.Vec[Token].new(),
    diags = diags,
  )
.end

# -----------------------------------------------------------------------------
# API publique
# -----------------------------------------------------------------------------

fn lex(source: String, file: String, diags: &mut diag.DiagnosticBag) -> coll.Vec[Token]
  let lexer = Lexer.new(source, file, diags)
  return lexer.run()
.end

# -----------------------------------------------------------------------------
# Implémentation – boucle principale
# -----------------------------------------------------------------------------

fn Lexer.run(self: &mut Lexer) -> coll.Vec[Token]
  while not self.is_eof()
    self.lex_one_token()
  end

  # Ajouter un token EOF final avec un span factice
  let eof_span = self.make_span_from_offsets(self.index, self.index)
  let eof = Token.new(TokenKind.Eof, "", eof_span)
  self.tokens.push(eof)

  return self.tokens
.end

fn Lexer.is_eof(self: Lexer) -> Bool
  return self.index >= self.source.len()
.end

# -----------------------------------------------------------------------------
# Helpers de navigation dans le texte source
# -----------------------------------------------------------------------------

fn Lexer.peek_char(self: Lexer) -> Char
  if self.is_eof()
    return '\0'
  end
  return self.source[self.index]
.end

fn Lexer.peek_next_char(self: Lexer) -> Char
  let next_index = self.index + 1
  if next_index >= self.source.len()
    return '\0'
  end
  return self.source[next_index]
.end

fn Lexer.advance(self: &mut Lexer) -> Char
  if self.is_eof()
    return '\0'
  end

  let c = self.source[self.index]
  self.index = self.index + 1
  return c
.end

fn Lexer.push_token(self: &mut Lexer, kind: TokenKind, lexeme: String, span: diag.Span) -> Unit
  let tok = Token.new(kind, lexeme, span)
  self.tokens.push(tok)
.end

fn Lexer.make_span_from_offsets(self: Lexer, start_offset: Int, end_offset: Int) -> diag.Span
  return self.line_index.span_for_range(start_offset, end_offset)
.end

fn Lexer.skip_line_comment(self: &mut Lexer) -> Unit
  while not self.is_eof()
    let c = self.peek_char()
    if c == '\n'
      break
    end
    self.advance()
  end
.end

# -----------------------------------------------------------------------------
# Lexing de base – version minimale à enrichir
# -----------------------------------------------------------------------------

fn Lexer.lex_one_token(self: &mut Lexer) -> Unit
  # NOTE :
  #   Implémentation plus complète :
  #   - saute les espaces simples,
  #   - matérialise les newlines,
  #   - détecte identifiants / mots-clés,
  #   - détecte nombres simples,
  #   - détecte chaînes basiques,
  #   - sinon, délègue aux opérateurs / ponctuation.

  let c = self.peek_char()

  if c == '\0'
    # Rien à faire, EOF sera ajouté dans run().
    return
  end

  # Espaces simples (hors newline) : on avance sans token.
  if c == ' ' or c == '\t' or c == '\r'
    self.advance()
    return
  end

  # Commentaires commençant par '#'
  if c == '#'
    self.skip_line_comment()
    return
  end

  # Newline -> token Newline
  if c == '\n'
    let start = self.index
    self.advance()
    let span = self.make_span_from_offsets(start, self.index)
    self.push_token(TokenKind.Newline, "\n", span)
    self.handle_indent_after_newline()
    return
  end

  # Identifiants / mots-clés
  if is_ident_start(c)
    self.lex_ident_or_keyword()
    return
  end

  # Nombres
  if is_digit(c)
    self.lex_number()
    return
  end

  # Caractères
  if c == '\''
    self.lex_char_literal()
    return
  end

  # Chaînes
  if c == '"'
    self.lex_string()
    return
  end

  # Opérateurs / ponctuations
  self.lex_operator_or_punct()
.end

# -----------------------------------------------------------------------------
# Hooks futurs – squelette pour lex ident / nombre / string
# -----------------------------------------------------------------------------

fn Lexer.lex_ident_or_keyword(self: &mut Lexer) -> Unit
  let start_offset = self.index

  let buf = ""
  while true
    let c = self.peek_char()
    if not is_ident_continue(c)
      break
    end
    let ch = self.advance()
    buf = buf + str.from_char(ch)
  end

  let span = self.make_span_from_offsets(start_offset, self.index)

  # Classification en mots-clés ou Ident
  let kind =
    if buf == "module" then TokenKind.KwModule
    else if buf == "import" then TokenKind.KwImport
    else if buf == "export" then TokenKind.KwExport
    else if buf == "struct" then TokenKind.KwStruct
    else if buf == "enum" then TokenKind.KwEnum
    else if buf == "union" then TokenKind.KwUnion
    else if buf == "type" then TokenKind.KwType
    else if buf == "fn" then TokenKind.KwFn
    else if buf == "program" then TokenKind.KwProgram
    else if buf == "scenario" then TokenKind.KwScenario
    else if buf == "pipeline" then TokenKind.KwPipeline
    else if buf == "end" then TokenKind.KwEnd
    else if buf == "let" then TokenKind.KwLet
    else if buf == "mut" then TokenKind.KwMut
    else if buf == "if" then TokenKind.KwIf
    else if buf == "else" then TokenKind.KwElse
    else if buf == "while" then TokenKind.KwWhile
    else if buf == "match" then TokenKind.KwMatch
    else if buf == "true" then TokenKind.KwTrue
    else if buf == "false" then TokenKind.KwFalse
    else TokenKind.Ident
    end

  self.push_token(kind, buf, span)
.end

fn Lexer.lex_number(self: &mut Lexer) -> Unit
  let start_offset = self.index

  let buf = ""
  let seen_dot = false

  while true
    let c = self.peek_char()
    if is_digit(c)
      let ch = self.advance()
      buf = buf + str.from_char(ch)
      continue
    end

    if c == '.' and not seen_dot
      # Premier '.' : on le considère comme flottant
      let ch = self.advance()
      buf = buf + str.from_char(ch)
      seen_dot = true
      continue
    end

    break
  end

  let span = self.make_span_from_offsets(start_offset, self.index)

  let kind =
    if seen_dot then TokenKind.FloatLiteral else TokenKind.IntLiteral end

  self.push_token(kind, buf, span)
.end

fn Lexer.lex_char_literal(self: &mut Lexer) -> Unit
  let start_offset = self.index

  # Consommer la quote ouvrante
  let _ = self.advance()

  let buf = ""
  let terminated = false

  if self.is_eof()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.diags.add_error(
      "Litt?ral de caract?re non termin?",
      span,
      diag.code_for_message("Litt?ral de caract?re non termin?"),
    )
    self.push_token(TokenKind.CharLiteral, buf, span)
    return
  end

  let c = self.peek_char()

  # Support d'une forme minimale : un caract?re ou une s?quence d'?chappement simple.
  if c == '\'
    let first = self.advance()
    buf = buf + str.from_char(first)
    if not self.is_eof()
      let escaped = self.advance()
      buf = buf + str.from_char(escaped)
    end
  else if c == '
' or c == ' '
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.diags.add_error(
      "Litt?ral de caract?re non termin?",
      span,
      diag.code_for_message("Litt?ral de caract?re non termin?"),
    )
    self.push_token(TokenKind.CharLiteral, buf, span)
    return
  else
    let ch = self.advance()
    buf = buf + str.from_char(ch)
  end

  if self.peek_char() == '''
    self.advance()
    terminated = true
  end

  let span = self.make_span_from_offsets(start_offset, self.index)

  if not terminated
    self.diags.add_error(
      "Litt?ral de caract?re non termin?",
      span,
      diag.code_for_message("Litt?ral de caract?re non termin?"),
    )
  end

  self.push_token(TokenKind.CharLiteral, buf, span)
.end

fn Lexer.lex_string(self: &mut Lexer) -> Unit
  let start_offset = self.index

  # Consommer le guillemet ouvrant
  let _ = self.advance()

  let buf = ""
  let terminated = false

  while not self.is_eof()
    let c = self.peek_char()

    if c == '"'
      # Fin de cha?ne
      self.advance()
      terminated = true
      break
    end

    if c == '
'
      # Fin de ligne avant la fermeture : erreur
      break
    end

    let ch = self.advance()
    buf = buf + str.from_char(ch)
  end

  let span = self.make_span_from_offsets(start_offset, self.index)

  if not terminated
    self.diags.add_error(
      "Litt?ral de cha?ne non termin?e",
      span,
      diag.code_for_message("Litt?ral de cha?ne non termin?e"),
    )
  end

  self.push_token(TokenKind.StringLiteral, buf, span)
.end

fn Lexer.lex_operator_or_punct(self: &mut Lexer) -> Unit
  let start_offset = self.index
  let c = self.advance()
  let next = self.peek_char()

  # 2-caract?res d'abord
  if c == '-' and next == '>'
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.Arrow, "->", span)
    return
  end

  if c == '=' and next == '>'
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.FatArrow, "=>", span)
    return
  end

  if c == '=' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.EqualEqual, "==", span)
    return
  end

  if c == '!' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.BangEqual, "!=", span)
    return
  end

  if c == '+' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.PlusEqual, "+=", span)
    return
  end

  if c == '-' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.MinusEqual, "-=", span)
    return
  end

  if c == '*' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.StarEqual, "*=", span)
    return
  end

  if c == '/' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.SlashEqual, "/=", span)
    return
  end

  if c == '!' and next != '='
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.Bang, "!", span)
    return
  end

  if c == '<' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.LessEqual, "<=", span)
    return
  end

  if c == '>' and next == '='
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.GreaterEqual, ">=", span)
    return
  end

  if c == '&' and next == '&'
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.AmpAmp, "&&", span)
    return
  end

  if c == '|' and next == '|'
    self.advance()
    let span = self.make_span_from_offsets(start_offset, self.index)
    self.push_token(TokenKind.PipePipe, "||", span)
    return
  end

  # Sinon, 1-caract?re
  let span = self.make_span_from_offsets(start_offset, self.index)

  if c == '('
    self.push_token(TokenKind.LParen, "(", span)
    return
  end

  if c == ')'
    self.push_token(TokenKind.RParen, ")", span)
    return
  end

  if c == '{'
    self.push_token(TokenKind.LBrace, "{", span)
    return
  end

  if c == '}'
    self.push_token(TokenKind.RBrace, "}", span)
    return
  end

  if c == '['
    self.push_token(TokenKind.LBracket, "[", span)
    return
  end

  if c == ']'
    self.push_token(TokenKind.RBracket, "]", span)
    return
  end

  if c == ','
    self.push_token(TokenKind.Comma, ",", span)
    return
  end

  if c == ':'
    if next == ':'
      self.advance()
      let wide_span = self.make_span_from_offsets(start_offset, self.index)
      self.push_token(TokenKind.ColonColon, "::", wide_span)
      return
    end
    self.push_token(TokenKind.Colon, ":", span)
    return
  end

  if c == '.'
    self.push_token(TokenKind.Dot, ".", span)
    return
  end

  if c == '='
    self.push_token(TokenKind.Equal, "=", span)
    return
  end

  if c == '+'
    self.push_token(TokenKind.Plus, "+", span)
    return
  end

  if c == '-'
    self.push_token(TokenKind.Minus, "-", span)
    return
  end

  if c == '*'
    self.push_token(TokenKind.Star, "*", span)
    return
  end

  if c == '/'
    self.push_token(TokenKind.Slash, "/", span)
    return
  end

  if c == '%'
    self.push_token(TokenKind.Percent, "%", span)
    return
  end

  if c == '<'
    self.push_token(TokenKind.Less, "<", span)
    return
  end

  if c == '>'
    self.push_token(TokenKind.Greater, ">", span)
    return
  end

  # Caract?re inconnu : produire Unknown et signaler imm?diatement.
  let s = str.from_char(c)
  self.push_token(TokenKind.Unknown, s, span)
  self.diags.add_error(
    "Caract?re invalide",
    span,
    diag.code_for_message("Caract?re invalide"),
  )
.end

fn Lexer.handle_indent_after_newline(self: &mut Lexer) -> Unit
  # Placeholder : on pourra ici mesurer l'indentation et émettre Indent/Dedent.
  return
.end

# -----------------------------------------------------------------------------
# Helpers de classification de caractères
# -----------------------------------------------------------------------------

fn is_alpha(c: Char) -> Bool
  return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z')
.end

fn is_digit(c: Char) -> Bool
  return c >= '0' and c <= '9'
.end

fn is_alnum(c: Char) -> Bool
  return is_alpha(c) or is_digit(c)
.end

fn is_ident_start(c: Char) -> Bool
  return is_alpha(c) or c == '_'
.end

fn is_ident_continue(c: Char) -> Bool
  return is_alnum(c) or c == '_'
.end
