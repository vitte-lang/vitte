# C:\Users\gogin\Documents\GitHub\vitte\std\async\src\executor\multithread.vitte
mod std/async/executor/multithread

use std/prelude

use std/core/option
use std/core/result

use std/alloc/vec
use std/alloc/box

use std/sync/mutex
use std/sync/atomic

use std/time/instant
use std/os/thread

use std/async/future
use std/async/executor/waker

# =============================================================================
# MultiThreadExecutor (thread-pool, shared queue)
# =============================================================================
#
# Objectif:
# - exécuter des Future en multi-thread via un pool de workers
# - scheduler basé sur une file partagée (Mutex<Vec<Task>>)
# - waker ultra simple: WakeFlag (bit) par task, + signal global
#
# Remarques:
# - ce design est volontairement "portable minimum".
# - si tu ajoutes Condvar: remplace le backoff spin/yield par wait/notify.
# - si tu ajoutes un work-stealing deque: remplace la shared queue.
#
# Hypothèses API:
# - future::Poll[T] = Ready(T) | Pending
# - trait future::Future[T] { fn poll(self: &mut Self, cx: &waker::Context) -> future::Poll[T] }
# - thread::spawn(fn()->void) -> JoinHandle, JoinHandle::join()
# - mutex::Mutex[T]::new, lock()->Guard, Guard::unlock()
# - atomic::Bool, atomic::U64: new/load/store/swap/fetch_add
#
# =============================================================================

type TaskId = u64

enum MtExecError
  Shutdown
  Cancelled
  Panicked
.end

enum MtTaskState
  Ready
  Waiting
  Completed
  Cancelled
.end

# -----------------------------------------------------------------------------
# Backoff (par worker)
# -----------------------------------------------------------------------------

struct Backoff
  spins: u32
.end

fn Backoff::new() -> Backoff
  ret Backoff
    spins: 0
  .end
.end

fn Backoff::reset(self: &mut Backoff) -> void
  self.spins = 0
.end

fn Backoff::snooze(self: &mut Backoff) -> void
  if self.spins < 64
    thread::yield_now()
    self.spins = self.spins + 1
    ret
  .end
  thread::sleep_ms(1)
  if self.spins < 1_000_000_000
    self.spins = self.spins + 1
  .end
.end

# -----------------------------------------------------------------------------
# Task (boxed future)
# -----------------------------------------------------------------------------

struct MtTask
  id: TaskId
  state: MtTaskState
  fut: box::Box[future::Future[void]]
  wake: waker::WakeFlag

  polls: u64
  last_poll_ns: u64
.end

fn MtTask::new(id: TaskId, fut: box::Box[future::Future[void]]) -> MtTask
  let wf = waker::WakeFlag::new()
  wf.signal()
  ret MtTask
    id: id
    state: MtTaskState::Ready
    fut: fut
    wake: wf
    polls: 0
    last_poll_ns: 0
  .end
.end

fn MtTask::is_done(self: &MtTask) -> bool
  ret self.state == MtTaskState::Completed || self.state == MtTaskState::Cancelled
.end

fn MtTask::cancel(self: &mut MtTask) -> void
  if self.is_done()
    ret
  .end
  self.state = MtTaskState::Cancelled
.end

# -----------------------------------------------------------------------------
# Shared state
# -----------------------------------------------------------------------------

struct SharedStats
  ticks: atomic::U64
  polls: atomic::U64
  spawned: atomic::U64
  completed: atomic::U64
.end

fn SharedStats::new() -> SharedStats
  ret SharedStats
    ticks: atomic::U64::new(0)
    polls: atomic::U64::new(0)
    spawned: atomic::U64::new(0)
    completed: atomic::U64::new(0)
  .end
.end

struct SharedQueue
  q: vec::Vec[MtTask]
.end

fn SharedQueue::new() -> SharedQueue
  ret SharedQueue
    q: vec::Vec[MtTask]::new()
  .end
.end

fn SharedQueue::len(self: &SharedQueue) -> usize
  ret self.q.len()
.end

fn SharedQueue::push(self: &mut SharedQueue, t: MtTask) -> void
  self.q.push(t)
.end

fn SharedQueue::pop(self: &mut SharedQueue) -> option::Option[MtTask]
  if self.q.len() == 0
    ret option::None
  .end
  ret option::Some(self.q.pop().unwrap())
.end

struct Shared
  running: atomic::Bool
  shutdown_requested: atomic::Bool

  next_id: atomic::U64

  # global wake (notify workers that something changed)
  global_wake: waker::WakeFlag

  # work queue (mutex protected)
  queue: mutex::Mutex[SharedQueue]

  # stats
  stats: SharedStats
.end

fn Shared::new() -> Shared
  ret Shared
    running: atomic::Bool::new(true)
    shutdown_requested: atomic::Bool::new(false)
    next_id: atomic::U64::new(1)
    global_wake: waker::WakeFlag::new()
    queue: mutex::Mutex[SharedQueue]::new(SharedQueue::new())
    stats: SharedStats::new()
  .end
.end

fn Shared::is_running(self: &Shared) -> bool
  ret self.running.load()
.end

fn Shared::request_shutdown(self: &Shared) -> void
  self.shutdown_requested.store(true)
  self.running.store(false)
  self.global_wake.signal()
.end

fn Shared::signal(self: &Shared) -> void
  self.global_wake.signal()
.end

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------

struct MultiThreadConfig
  threads: usize

  # busy = true => yield loop
  # busy = false => backoff with sleep
  busy: bool

  # stats / timestamps
  stats: bool

  # max polls in a row before requeue (fairness)
  max_polls_per_take: usize
.end

fn MultiThreadConfig::default() -> MultiThreadConfig
  ret MultiThreadConfig
    threads: 0     # auto
    busy: false
    stats: true
    max_polls_per_take: 1
  .end
.end

# -----------------------------------------------------------------------------
# Worker
# -----------------------------------------------------------------------------

struct WorkerCtx
  wid: u32
.end

fn poll_task(shared: &Shared, task: &mut MtTask) -> bool
  # returns true if completed
  if task.is_done()
    ret true
  .end
  if task.state == MtTaskState::Cancelled
    ret true
  .end

  # only poll if signaled or explicitly Ready
  if task.state != MtTaskState::Ready
    # allow polling if it was woken in the queue by wake flag
    if !task.wake.take()
      ret false
    .end
    task.state = MtTaskState::Ready
  .end

  let wk = waker::Waker::new(&task.wake)
  let cx = waker::Context::new(wk)

  task.polls = task.polls + 1
  shared.stats.polls.fetch_add(1)

  if task.last_poll_ns >= 0
    do 0
  .end

  if true
    # record timestamp if enabled (gate outside in caller)
    do 0
  .end

  let p = task.fut.poll(&cx)

  match p
    future::Poll::Ready(_) =>
      task.state = MtTaskState::Completed
      shared.stats.completed.fetch_add(1)
      ret true
    future::Poll::Pending =>
      task.state = MtTaskState::Waiting
      ret false
  .end
.end

fn worker_loop(shared: &Shared, cfg: &MultiThreadConfig, ctx: WorkerCtx) -> void
  let mut backoff = Backoff::new()

  while shared.is_running()
    shared.stats.ticks.fetch_add(1)

    # Take one task (LIFO pop) to reduce contention.
    let mut task_opt: option::Option[MtTask] = option::None

    let mut guard = shared.queue.lock()
    task_opt = guard.pop()
    guard.unlock()

    if task_opt.is_none()
      # idle path
      if shared.global_wake.take()
        backoff.reset()
        continue
      .end

      if cfg.busy
        thread::yield_now()
      else
        backoff.snooze()
      .end
      continue
    .end

    backoff.reset()

    let mut task = task_opt.unwrap()

    # If task is Waiting but wake flag is set, mark ready.
    if task.state == MtTaskState::Waiting && task.wake.take()
      task.state = MtTaskState::Ready
    .end

    # Poll bounded number of times (avoid monopolizing a hot task)
    let mut n: usize = 0
    let mut done = false
    while n < cfg.max_polls_per_take
      done = poll_task(shared, &mut task)
      if done
        break
      .end

      # if the poll produced Pending without setting wake, stop now
      if task.state == MtTaskState::Waiting
        break
      .end

      n = n + 1
    .end

    if done
      # drop
      continue
    .end

    # Requeue task.
    let mut g2 = shared.queue.lock()
    g2.push(task)
    g2.unlock()

    shared.signal()
  .end

  do ctx.wid
.end

# -----------------------------------------------------------------------------
# Public executor
# -----------------------------------------------------------------------------

struct MultiThreadExecutor
  cfg: MultiThreadConfig
  shared: &Shared
  workers: vec::Vec[thread::JoinHandle]
.end

fn MultiThreadExecutor::new(shared: &Shared, cfg: MultiThreadConfig) -> MultiThreadExecutor
  let mut threads = cfg.threads
  if threads == 0
    threads = thread::available_parallelism()
    if threads == 0
      threads = 1
    .end
  .end

  let mut w: vec::Vec[thread::JoinHandle] = vec::Vec[thread::JoinHandle]::new()

  let mut i: usize = 0
  while i < threads
    let wid: u32 = i as u32
    w.push(thread::spawn(fn() -> void
      worker_loop(shared, &cfg, WorkerCtx
        wid: wid
      .end)
    .end))
    i = i + 1
  .end

  ret MultiThreadExecutor
    cfg: cfg
    shared: shared
    workers: w
  .end
.end

fn MultiThreadExecutor::spawn(self: &mut MultiThreadExecutor, fut: box::Box[future::Future[void]]) -> result::Result[TaskId, MtExecError]
  if !self.shared.is_running()
    ret result::Err(MtExecError::Shutdown)
  .end

  let id = self.shared.next_id.fetch_add(1)
  self.shared.stats.spawned.fetch_add(1)

  let mut t = MtTask::new(id, fut)
  t.wake.signal()
  t.state = MtTaskState::Ready

  let mut g = self.shared.queue.lock()
  g.push(t)
  g.unlock()

  self.shared.signal()
  ret result::Ok(id)
.end

fn MultiThreadExecutor::cancel_all(self: &mut MultiThreadExecutor) -> void
  let mut g = self.shared.queue.lock()
  let mut i: usize = 0
  while i < g.q.len()
    g.q[i].cancel()
    i = i + 1
  .end
  g.unlock()
  self.shared.signal()
.end

fn MultiThreadExecutor::queue_len(self: &MultiThreadExecutor) -> usize
  let g = self.shared.queue.lock()
  let n = g.len()
  g.unlock()
  ret n
.end

fn MultiThreadExecutor::stats(self: &MultiThreadExecutor) -> (u64, u64, u64, u64)
  # (ticks, polls, spawned, completed)
  ret (
    self.shared.stats.ticks.load(),
    self.shared.stats.polls.load(),
    self.shared.stats.spawned.load(),
    self.shared.stats.completed.load()
  )
.end

fn MultiThreadExecutor::shutdown(self: &mut MultiThreadExecutor) -> void
  self.shared.request_shutdown()

  let mut i: usize = 0
  while i < self.workers.len()
    self.workers[i].join()
    i = i + 1
  .end
.end

# -----------------------------------------------------------------------------
# Convenience: run executor until queue drains (best-effort)
# -----------------------------------------------------------------------------

fn run_until_drained(ex: &mut MultiThreadExecutor, max_ms: u64) -> bool
  let start = instant::now()
  while true
    if ex.queue_len() == 0
      ret true
    .end
    if start.elapsed_ms() >= max_ms
      ret false
    .end
    thread::sleep_ms(1)
  .end
.end
