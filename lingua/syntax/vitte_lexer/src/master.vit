# File: C:\Users\gogin\Documents\GitHub\vitte\lingua\syntax\vitte_lexer\src\master.vit
space lingua/syntax/vitte_lexer/master

<<< master
  vitte_lexer/master.vit â€” Lexer (MAXMAX)

  Purpose:
    - Canonical lexer for Vitte "vitte vit" syntax.
    - Produces a token stream with:
        * kinds, text/slice, span (byte offsets), line/col map
        * trivia handling: whitespace, line comments "#", doc zones "<<< >>>"
        * numeric literals: 0b/0o/0x, underscores, suffixes (i*/u*/isize/usize, f32/f64)
        * strings: "..." and '...' with escapes (\n \r \t \0 \\ \" \' \uXXXX)
        * identifiers: [_A-Za-z][_A-Za-z0-9]*
        * keywords: vitte, doc, space, pull, share, build, pub, hid, form, pick, bond,
                    field, case, const, var, proc, flow, entry, app, service, tool,
                    pipeline, driver, kernel, make, keep, set, if, elif, else, loop,
                    while, until, each, in, select, when, otherwise, give, emit, defer,
                    assert, foreign, abi, true, false, null, or, and, not, List, Map,
                    Pack, of, to, mark, as, means
        * punctuation/operators: (), [], {}, <<<, >>>, ., .., ::, /, , , :, ;, =>,
                                 =, +=, -=, *=, /=, %=, ==, !=, <, <=, >, >=,
                                 +, -, *, /, %, !
        * NEWLINE tokens and indentation tracking (optional, but emitted as trivia by default)

  Notes:
    - This file is self-contained at algorithm level; replace std stubs with your real std modules.
    - Blocks end ONLY with ".end"; lexer must return ".end" as keyword? Here it is tokenized as IDENT unless you add it as keyword.
    - Curly braces may exist in map literals in older grammar; lexer supports { } but parser can forbid.
>>>

pull std/text as text
pull std/collections/list as list
pull std/collections/map as map

pull lingua/syntax/vitte_ast/span as span
pull lingua/syntax/vitte_ast/diag as diag

share all

bond Text means String


<<< =========================================================
  0) PUBLIC TOKEN MODEL
========================================================= >>>

pick TokKind
case Eof()

# trivia
case Whitespace()
case Newline()
case CommentLine()        # "# ...\n"
case DocLine()            # "doc ..."
case DocZoneStart()       # "<<<"
case DocZoneEnd()         # ">>>"
case DocZoneText()        # raw lines inside zone (optional mode)

# identifiers + keywords
case Ident()
case Keyword()

# literals
case IntLit()
case FloatLit()
case StringLit()
case BoolLit()
case NullLit()

# punctuation / operators
case Punct()              # single-char punct or multi-char operators
.end

form Token
field kind as TokKind = TokKind.Eof()
field text as Text = ""
field span as span.Span = span.Span()
field line as Int = 1
field col as Int = 1
.end

form LexOptions
field keep_trivia as Bool = true
field emit_newlines as Bool = true
field collapse_whitespace as Bool = true
field doc_zone_as_tokens as Bool = true     # if false, doc zones are lexed as comments
field max_token_len as Int = 1_048_576
.end

proc lex_options_default() gives LexOptions
give LexOptions()
.end


<<< =========================================================
  1) LEXER STATE
========================================================= >>>

form Lexer
field src as Text = ""
field len as Int = 0
field i as Int = 0

# line tracking
field line as Int = 1
field col as Int = 1
field line_start as Int = 0

field opts as LexOptions = LexOptions()

# diagnostics (optional)
field diags as List of diag.Diagnostic = []
.end

proc lexer_new(src as Text, opts as LexOptions) gives Lexer
make l as Lexer = Lexer()
set l.src = src
set l.len = text.len(src)
set l.i = 0
set l.line = 1
set l.col = 1
set l.line_start = 0
set l.opts = opts
set l.diags = []
give l
.end

proc lexer_diags(l as Lexer) gives List of diag.Diagnostic
give l.diags
.end


<<< =========================================================
  2) MAIN API
========================================================= >>>

proc lex_all(src as Text, opts as LexOptions, diags as List of diag.Diagnostic) gives List of Token
make lx as Lexer = lexer_new(src, opts)
set lx.diags = diags

make out as List of Token = []
loop while true
  make t as Token = lexer_next(lx)
  if t.kind is TokKind.Eof()
    list.push(out, t)
    loop while false .end
  .end
  list.push(out, t)
.end
give out
.end

proc lexer_next(lx as Lexer) gives Token
_skip_trivia(lx)

if lx.i >= lx.len
  give _tok(lx, TokKind.Eof(), "", lx.i, lx.i)
.end

# doc zone: <<< ... >>>
if _starts_with(lx, "<<<")
  give _lex_doc_zone(lx)
.end

# newline
if _peek_is_nl(lx)
  give _lex_newline(lx)
.end

make ch as Text = _peek(lx)

# identifier / keyword
if _is_ident_start(ch)
  give _lex_ident_or_keyword(lx)
.end

# number: digit or '.' followed by digit (float leading dot)
if _is_digit(ch)
  give _lex_number(lx)
.end
if ch == "."
  if _peek2_is_digit(lx)
    give _lex_number(lx)
  .end
.end

# string literals
if ch == "\""
  give _lex_string(lx, "\"")
.end
if ch == "'"
  give _lex_string(lx, "'")
.end

# operators/punctuations (max munch)
give _lex_punct(lx)
.end


<<< =========================================================
  3) TRIVIA SKIPPER
========================================================= >>>

proc _skip_trivia(lx as Lexer)
# Skip whitespace and comments depending on options.
loop while true
  if lx.i >= lx.len
    give
  .end

  # newline is not skipped if emit_newlines
  if _peek_is_nl(lx)
    if lx.opts.emit_newlines
      give
    .end
    _consume_newline(lx)
    loop while false .end
  .end

  # spaces/tabs
  if _peek_is_ws(lx)
    if lx.opts.keep_trivia
      give
    .end
    _consume_ws(lx)
    loop while false .end
  .end

  # line comment "#"
  if _peek(lx) == "#"
    if lx.opts.keep_trivia
      give
    .end
    _consume_line_comment(lx)
    loop while false .end
  .end

  give
.end
.end


<<< =========================================================
  4) LEX NEWLINE / WHITESPACE / COMMENT
========================================================= >>>

proc _lex_newline(lx as Lexer) gives Token
make start as Int = lx.i
_consume_newline(lx)
make end as Int = lx.i
give _tok(lx, TokKind.Newline(), "\n", start, end)
.end

proc _lex_ws(lx as Lexer) gives Token
make start as Int = lx.i
_consume_ws(lx)
make end as Int = lx.i
make s as Text = text.slice(lx.src, start, end)
if lx.opts.collapse_whitespace
  set s = " "
.end
give _tok(lx, TokKind.Whitespace(), s, start, end)
.end

proc _lex_line_comment(lx as Lexer) gives Token
make start as Int = lx.i
_consume_line_comment(lx)
make end as Int = lx.i
make s as Text = text.slice(lx.src, start, end)
give _tok(lx, TokKind.CommentLine(), s, start, end)
.end

proc _consume_ws(lx as Lexer)
loop while lx.i < lx.len
  make c as Text = _peek(lx)
  if c == " " or c == "\t"
    _adv(lx, 1)
  else
    give
  .end
.end
.end

proc _consume_newline(lx as Lexer)
# Supports \n or \r\n
if lx.i < lx.len and text.at(lx.src, lx.i) == "\r"
  if lx.i + 1 < lx.len and text.at(lx.src, lx.i + 1) == "\n"
    _adv(lx, 2)
  else
    _adv(lx, 1)
  .end
else
  _adv(lx, 1)
.end

set lx.line = lx.line + 1
set lx.col = 1
set lx.line_start = lx.i
.end

proc _consume_line_comment(lx as Lexer)
# consume '#' then until newline or EOF
_adv(lx, 1)
loop while lx.i < lx.len
  if _peek_is_nl(lx)
    give
  .end
  _adv(lx, 1)
.end
.end


<<< =========================================================
  5) DOC ZONE LEXER (<<< ... >>>)
========================================================= >>>

proc _lex_doc_zone(lx as Lexer) gives Token
# Emits DocZoneStart and DocZoneEnd (and optionally DocZoneText) depending on mode.
# This function returns the first token (DocZoneStart) and pushes the rest into an internal queue?
# For simplicity in MAXMAX, we lex whole zone and return a single CommentLine-like token if not tokenized.
if lx.opts.doc_zone_as_tokens == false
  make start as Int = lx.i
  _consume_doc_zone(lx)
  make end as Int = lx.i
  make s as Text = text.slice(lx.src, start, end)
  give _tok(lx, TokKind.CommentLine(), s, start, end)
.end

# Tokenized mode: emit DocZoneStart now, and leave lexer positioned after "<<<".
make start as Int = lx.i
_adv(lx, 3)
make end as Int = lx.i
give _tok(lx, TokKind.DocZoneStart(), "<<<", start, end)
.end

proc _consume_doc_zone(lx as Lexer)
# consume "<<<" then everything until ">>>" (inclusive) or EOF
_adv(lx, 3)
loop while lx.i < lx.len
  if _starts_with(lx, ">>>")
    _adv(lx, 3)
    give
  .end
  if _peek_is_nl(lx)
    _consume_newline(lx)
  else
    _adv(lx, 1)
  .end
.end
.end


<<< =========================================================
  6) IDENTIFIERS / KEYWORDS
========================================================= >>>

proc _lex_ident_or_keyword(lx as Lexer) gives Token
make start as Int = lx.i
_adv(lx, 1)
loop while lx.i < lx.len
  make c as Text = _peek(lx)
  if _is_ident_continue(c)
    _adv(lx, 1)
  else
    loop while false .end
  .end
.end
make end as Int = lx.i
make s as Text = text.slice(lx.src, start, end)

# Keyword classification
if _is_keyword(s)
  # true/false/null classified as BoolLit/NullLit for convenience
  if s == "true" or s == "false"
    give _tok(lx, TokKind.BoolLit(), s, start, end)
  .end
  if s == "null"
    give _tok(lx, TokKind.NullLit(), s, start, end)
  .end
  give _tok(lx, TokKind.Keyword(), s, start, end)
.end

give _tok(lx, TokKind.Ident(), s, start, end)
.end

proc _is_keyword(s as Text) gives Bool
# Header / docs
if s == "vitte" give true .end
if s == "doc" give true .end

# modules
if s == "space" give true .end
if s == "pull" give true .end
if s == "share" give true .end
if s == "build" give true .end
if s == "as" give true .end
if s == "only" give true .end

# visibility
if s == "pub" give true .end
if s == "hid" give true .end

# types
if s == "form" give true .end
if s == "pick" give true .end
if s == "bond" give true .end
if s == "field" give true .end
if s == "case" give true .end
if s == "means" give true .end

# globals
if s == "const" give true .end
if s == "var" give true .end

# runnables
if s == "proc" give true .end
if s == "flow" give true .end
if s == "entry" give true .end
if s == "app" give true .end
if s == "service" give true .end
if s == "tool" give true .end
if s == "pipeline" give true .end
if s == "driver" give true .end
if s == "kernel" give true .end
if s == "mark" give true .end

# statements
if s == "make" give true .end
if s == "keep" give true .end
if s == "set" give true .end
if s == "if" give true .end
if s == "elif" give true .end
if s == "else" give true .end
if s == "loop" give true .end
if s == "while" give true .end
if s == "until" give true .end
if s == "each" give true .end
if s == "in" give true .end
if s == "select" give true .end
if s == "when" give true .end
if s == "otherwise" give true .end
if s == "give" give true .end
if s == "emit" give true .end
if s == "defer" give true .end
if s == "assert" give true .end
if s == "foreign" give true .end
if s == "abi" give true .end

# operators / logic
if s == "or" give true .end
if s == "and" give true .end
if s == "not" give true .end

# collection type spellings
if s == "List" give true .end
if s == "Map" give true .end
if s == "Pack" give true .end
if s == "of" give true .end
if s == "to" give true .end
if s == "true" give true .end
if s == "false" give true .end
if s == "null" give true .end

give false
.end


<<< =========================================================
  7) NUMBERS (INT/FLOAT) + SUFFIX
========================================================= >>>

proc _lex_number(lx as Lexer) gives Token
make start as Int = lx.i
make saw_dot as Bool = false
make saw_exp as Bool = false

# leading dot
if _peek(lx) == "."
  set saw_dot = true
  _adv(lx, 1)
.end

# base prefix (only if not leading dot)
if saw_dot == false
  if _starts_with(lx, "0b") or _starts_with(lx, "0o") or _starts_with(lx, "0x")
    _adv(lx, 2)
  .end
.end

# digits/underscores before dot/exp
loop while lx.i < lx.len
  make c as Text = _peek(lx)
  if _is_digit(c) or c == "_"
    _adv(lx, 1)
  else
    give _lex_number_after_int_part(lx, start, saw_dot, saw_exp)
  .end
.end

give _finish_number(lx, start, saw_dot, saw_exp)
.end

proc _lex_number_after_int_part(lx as Lexer, start as Int, saw_dot as Bool, saw_exp as Bool) gives Token
let mut dot = saw_dot
let mut exp = saw_exp

# dot fractional part (only if not already saw dot and next is '.')
if dot == false and _peek(lx) == "."
  if lx.i + 1 < lx.len and text.at(lx.src, lx.i + 1) != "."
    set dot = true
    _adv(lx, 1)
    loop while lx.i < lx.len
      make c as Text = _peek(lx)
      if _is_digit(c) or c == "_"
        _adv(lx, 1)
      else
        loop while false .end
      .end
    .end
  .end
.end

# exponent part
if lx.i < lx.len and (_peek(lx) == "e" or _peek(lx) == "E")
  set exp = true
  _adv(lx, 1)
  if lx.i < lx.len and (_peek(lx) == "+" or _peek(lx) == "-")
    _adv(lx, 1)
  .end
  loop while lx.i < lx.len
    make c as Text = _peek(lx)
    if _is_digit(c) or c == "_"
      _adv(lx, 1)
    else
      loop while false .end
    .end
  .end
.end

# suffix (letters/digits)
loop while lx.i < lx.len
  make c as Text = _peek(lx)
  if _is_ident_continue(c)
    _adv(lx, 1)
  else
    loop while false .end
  .end
.end

give _finish_number(lx, start, dot, exp)
.end

proc _finish_number(lx as Lexer, start as Int, saw_dot as Bool, saw_exp as Bool) gives Token
make end as Int = lx.i
make s as Text = text.slice(lx.src, start, end)

# classify: float if dot or exp or suffix f32/f64
if saw_dot or saw_exp or _ends_with_float_suffix(s)
  give _tok(lx, TokKind.FloatLit(), s, start, end)
.end
give _tok(lx, TokKind.IntLit(), s, start, end)
.end

proc _ends_with_float_suffix(s as Text) gives Bool
if text.ends_with(s, "f32") give true .end
if text.ends_with(s, "f64") give true .end
give false
.end


<<< =========================================================
  8) STRINGS (DQ/SQ) + ESCAPES
========================================================= >>>

proc _lex_string(lx as Lexer, quote as Text) gives Token
make start as Int = lx.i
_adv(lx, 1)

loop while lx.i < lx.len
  if _peek_is_nl(lx)
    list.push(lx.diags, diag.error("lexer-string-newline", _span(lx, start, lx.i)))
    loop while false .end
  .end

  make c as Text = _peek(lx)
  if c == quote
    _adv(lx, 1)
    make end as Int = lx.i
    make s as Text = text.slice(lx.src, start, end)
    give _tok(lx, TokKind.StringLit(), s, start, end)
  .end

  if c == "\\"
    _adv(lx, 1)
    if lx.i >= lx.len
      list.push(lx.diags, diag.error("lexer-string-eof", _span(lx, start, lx.i)))
      loop while false .end
    .end
    make e as Text = _peek(lx)
    if e == "u"
      _adv(lx, 1)
      make k as Int = 0
      loop while k < 4 and lx.i < lx.len
        if _is_hex(_peek(lx)) == false
          list.push(lx.diags, diag.error("lexer-string-bad-unicode", _span(lx, start, lx.i)))
          loop while false .end
        .end
        _adv(lx, 1)
        set k += 1
      .end
    else
      # simple escapes: \n\r\t\0\\\"\' 
      _adv(lx, 1)
    .end
    loop while false .end
  .end

  _adv(lx, 1)
.end

list.push(lx.diags, diag.error("lexer-string-eof", _span(lx, start, lx.i)))
make end2 as Int = lx.i
make s2 as Text = text.slice(lx.src, start, end2)
give _tok(lx, TokKind.StringLit(), s2, start, end2)
.end


<<< =========================================================
  9) PUNCTUATION / OPERATORS (MAX MUNCH)
========================================================= >>>

proc _lex_punct(lx as Lexer) gives Token
make start as Int = lx.i

# Multi-char first (longest match)
make op as Text = ""

if _starts_with(lx, ">>>") set op = ">>>" .end
if op == "" and _starts_with(lx, "<<<") set op = "<<<" .end
if op == "" and _starts_with(lx, "::") set op = "::" .end
if op == "" and _starts_with(lx, "=>") set op = "=>" .end
if op == "" and _starts_with(lx, "==") set op = "==" .end
if op == "" and _starts_with(lx, "!=") set op = "!=" .end
if op == "" and _starts_with(lx, "<=") set op = "<=" .end
if op == "" and _starts_with(lx, ">=") set op = ">=" .end
if op == "" and _starts_with(lx, "+=") set op = "+=" .end
if op == "" and _starts_with(lx, "-=") set op = "-=" .end
if op == "" and _starts_with(lx, "*=") set op = "*=" .end
if op == "" and _starts_with(lx, "/=") set op = "/=" .end
if op == "" and _starts_with(lx, "%=") set op = "%=" .end
if op == "" and _starts_with(lx, "..") set op = ".." .end

if op != ""
  _adv(lx, text.len(op))
  make end as Int = lx.i
  give _tok(lx, TokKind.Punct(), op, start, end)
.end

# single-char punct
make c as Text = _peek(lx)
_adv(lx, 1)
make end2 as Int = lx.i
give _tok(lx, TokKind.Punct(), c, start, end2)
.end


<<< =========================================================
  10) LOW-LEVEL SOURCE ACCESS
========================================================= >>>

proc _peek(lx as Lexer) gives Text
give text.at(lx.src, lx.i)
.end

proc _peek2_is_digit(lx as Lexer) gives Bool
if lx.i + 1 >= lx.len
  give false
.end
give _is_digit(text.at(lx.src, lx.i + 1))
.end

proc _peek_is_ws(lx as Lexer) gives Bool
if lx.i >= lx.len
  give false
.end
make c as Text = _peek(lx)
give c == " " or c == "\t"
.end

proc _peek_is_nl(lx as Lexer) gives Bool
if lx.i >= lx.len
  give false
.end
make c as Text = _peek(lx)
if c == "\n" give true .end
if c == "\r" give true .end
give false
.end

proc _starts_with(lx as Lexer, s as Text) gives Bool
give text.starts_with_at(lx.src, lx.i, s)
.end

proc _adv(lx as Lexer, n as Int)
set lx.i = lx.i + n
set lx.col = lx.col + n
.end

proc _span(lx as Lexer, lo as Int, hi as Int) gives span.Span
# Replace with real span construction
give span.from_offsets(lo, hi)
.end

proc _tok(lx as Lexer, k as TokKind, t as Text, lo as Int, hi as Int) gives Token
make tok as Token = Token()
set tok.kind = k
set tok.text = t
set tok.span = _span(lx, lo, hi)
set tok.line = lx.line
set tok.col = lx.col
give tok
.end


<<< =========================================================
  11) CHAR CLASSES
========================================================= >>>

proc _is_digit(c as Text) gives Bool
give c >= "0" and c <= "9"
.end

proc _is_hex(c as Text) gives Bool
if _is_digit(c) give true .end
if c >= "a" and c <= "f" give true .end
if c >= "A" and c <= "F" give true .end
give false
.end

proc _is_ident_start(c as Text) gives Bool
if c == "_" give true .end
if c >= "a" and c <= "z" give true .end
if c >= "A" and c <= "Z" give true .end
give false
.end

proc _is_ident_continue(c as Text) gives Bool
if _is_ident_start(c) give true .end
if _is_digit(c) give true .end
give false
.end
