

# vitte_beryl.impls
#
# Lexer, Tokenization, and Bytecode representation for Vitte.
#
# Design goals (max, self-contained):
# - Token types + token representation (with Pos/Span)
# - Lexer with: identifiers/keywords, numbers, strings, operators, delimiters
# - Line/column tracking
# - TokenBuffer for lookahead parsing
# - Bytecode + ops + instruction spans
#
# Notes:
# - This module intentionally carries minimal Option/Result/Vec utilities and
#   std_* stubs at the bottom to decouple from the current std state.
# - Contrainte: pas d’accolades, blocs `.end`.

module vitte_beryl.impls

# ============================================================================
# Minimal generics (Option/Result/Vec)
# ============================================================================

type Option[T] enum
  None
  Some(value: T)
.end

type Result[T, E] enum
  Ok(value: T)
  Err(error: E)
.end

fn opt_some[T](v: T) -> Option[T]
  ret Option::Some(v)
.end

fn opt_none[T]() -> Option[T]
  ret Option::None
.end

fn ok[T, E](v: T) -> Result[T, E]
  ret Result::Ok(v)
.end

fn err[T, E](e: E) -> Result[T, E]
  ret Result::Err(e)
.end

# Vec backed by array

type Vec[T] struct
  items: [T]
.end

fn vec_new[T]() -> Vec[T]
  let v: Vec[T]
  v.items = []
  ret v
.end

fn vec_len[T](v: Vec[T]) -> int
  ret std_len(v.items)
.end

fn vec_is_empty[T](v: Vec[T]) -> bool
  ret std_len(v.items) == 0
.end

fn vec_push[T](mut v: Vec[T], x: T)
  v.items = v.items + [x]
.end

fn vec_get[T](v: Vec[T], idx: int) -> Option[T]
  if idx < 0
    ret Option::None
  .end
  if idx >= std_len(v.items)
    ret Option::None
  .end
  ret Option::Some(v.items[idx])
.end

fn vec_set[T](mut v: Vec[T], idx: int, x: T) -> bool
  if idx < 0
    ret false
  .end
  if idx >= std_len(v.items)
    ret false
  .end
  v.items[idx] = x
  ret true
.end

# ============================================================================
# Positioning (Pos/Span)
# ============================================================================

type Pos struct
  offset: int
  line: int
  col: int
.end

fn pos_new(offset: int, line: int, col: int) -> Pos
  let p: Pos
  p.offset = offset
  p.line = line
  p.col = col
  ret p
.end

fn pos_zero() -> Pos
  ret pos_new(0, 1, 1)
.end

type Span struct
  start: Pos
  end: Pos
.end

fn span_new(a: Pos, b: Pos) -> Span
  let s: Span
  s.start = a
  s.end = b
  ret s
.end

fn span_empty() -> Span
  ret span_new(pos_zero(), pos_zero())
.end

fn span_merge(a: Span, b: Span) -> Span
  let s: Span
  s.start = a.start
  s.end = b.end
  ret s
.end

# ============================================================================
# Token model
# ============================================================================

type TokenType enum
  # special
  Eof
  Error
  Newline

  # identifiers / literals
  Ident
  Int
  Float
  Str

  # keywords (subset; extend)
  KwLet
  KwConst
  KwFn
  KwScn
  KwRet
  KwIf
  KwElif
  KwElse
  KwWhile
  KwFor
  KwIn
  KwMatch
  KwBreak
  KwContinue
  KwTrue
  KwFalse
  KwModule
  KwUse
  KwType
  KwStruct
  KwEnum

  # operators
  Eq
  EqEq
  Bang
  BangEq
  Plus
  Minus
  Star
  Slash
  Percent
  Lt
  Le
  Gt
  Ge
  AndAnd
  OrOr
  Amp
  Pipe
  Caret

  # punctuation
  Colon
  Semi
  Comma
  Dot
  Arrow

  # delimiters
  LParen
  RParen
  LBracket
  RBracket
  LBrace
  RBrace
.end

# Severity for errors (lexer)

type DiagLevel enum
  Note
  Warning
  Error
.end

type Diag struct
  level: DiagLevel
  message: str
  span: Span
.end

fn diag_error(msg: str, sp: Span) -> Diag
  let d: Diag
  d.level = DiagLevel::Error
  d.message = msg
  d.span = sp
  ret d
.end

# Token payload

type Token struct
  kind: TokenType
  lexeme: str
  span: Span
.end

fn token_new(kind: TokenType, lexeme: str, sp: Span) -> Token
  let t: Token
  t.kind = kind
  t.lexeme = lexeme
  t.span = sp
  ret t
.end

# ============================================================================
# Lexer
# ============================================================================

type LexerConfig struct
  keep_newlines: bool
  allow_hash_comments: bool
  allow_slash_comments: bool
.end

fn lexer_config_default() -> LexerConfig
  let c: LexerConfig
  c.keep_newlines = false
  c.allow_hash_comments = true
  c.allow_slash_comments = true
  ret c
.end

type Lexer struct
  src: str
  cfg: LexerConfig

  # cursor
  idx: int
  line: int
  col: int

  # diagnostics
  diags: Vec[Diag]
.end

fn lexer_new(src: str) -> Lexer
  let lx: Lexer
  lx.src = src
  lx.cfg = lexer_config_default()
  lx.idx = 0
  lx.line = 1
  lx.col = 1
  lx.diags = vec_new[Diag]()
  ret lx
.end

fn lexer_with_config(src: str, cfg: LexerConfig) -> Lexer
  let lx: Lexer
  lx.src = src
  lx.cfg = cfg
  lx.idx = 0
  lx.line = 1
  lx.col = 1
  lx.diags = vec_new[Diag]()
  ret lx
.end

fn lexer_diags(lx: Lexer) -> Vec[Diag]
  ret lx.diags
.end

fn lexer_tokenize(mut lx: Lexer) -> Vec[Token]
  let out = vec_new[Token]()

  while true
    let t = lexer_next_token(lx)

    # push token
    vec_push[Token](out, t)

    if t.kind == TokenType::Eof
      break
    .end
  .end

  ret out
.end

fn lexer_next_token(mut lx: Lexer) -> Token
  lexer_skip_ws_and_comments(lx)

  let start = lexer_pos(lx)

  # end
  if lexer_is_eof(lx)
    let sp = span_new(start, start)
    ret token_new(TokenType::Eof, "", sp)
  .end

  let ch = lexer_peek(lx)

  # newline handling
  if ch == "\n"
    lexer_advance(lx)
    let endp = lexer_pos(lx)
    let sp = span_new(start, endp)
    if lx.cfg.keep_newlines
      ret token_new(TokenType::Newline, "\n", sp)
    else
      # If we don't keep newlines, skip and continue
      ret lexer_next_token(lx)
    .end
  .end

  # identifier / keyword
  if is_ident_start(ch)
    ret lexer_lex_ident_or_keyword(lx)
  .end

  # number
  if is_digit(ch)
    ret lexer_lex_number(lx)
  .end

  # string literal
  if ch == "\""
    ret lexer_lex_string(lx)
  .end

  # operators / punctuation / delimiters
  ret lexer_lex_symbol(lx)
.end

fn lexer_pos(lx: Lexer) -> Pos
  ret pos_new(lx.idx, lx.line, lx.col)
.end

fn lexer_is_eof(lx: Lexer) -> bool
  ret lx.idx >= std_len_str(lx.src)
.end

fn lexer_peek(lx: Lexer) -> str
  if lexer_is_eof(lx)
    ret ""
  .end
  ret std_char_at_str(lx.src, lx.idx)
.end

fn lexer_peek_n(lx: Lexer, n: int) -> str
  let j = lx.idx + n
  if j < 0
    ret ""
  .end
  if j >= std_len_str(lx.src)
    ret ""
  .end
  ret std_char_at_str(lx.src, j)
.end

fn lexer_advance(mut lx: Lexer) -> str
  if lexer_is_eof(lx)
    ret ""
  .end

  let ch = std_char_at_str(lx.src, lx.idx)
  lx.idx = lx.idx + 1

  if ch == "\n"
    lx.line = lx.line + 1
    lx.col = 1
  else
    lx.col = lx.col + 1
  .end

  ret ch
.end

fn lexer_take_while(mut lx: Lexer, pred_name: str) -> str
  # pred_name est une clé ("ident", "digit", ...). évite closures pour rester portable.
  let start = lx.idx
  while not lexer_is_eof(lx)
    let ch = lexer_peek(lx)
    if pred_name == "ident_continue"
      if not is_ident_continue(ch)
        break
      .end
    elif pred_name == "digit"
      if not is_digit(ch)
        break
      .end
    elif pred_name == "hexdigit"
      if not is_hex_digit(ch)
        break
      .end
    else
      break
    .end

    lexer_advance(lx)
  .end

  let endi = lx.idx
  ret std_slice_str(lx.src, start, endi)
.end

fn lexer_skip_ws_and_comments(mut lx: Lexer)
  while true
    if lexer_is_eof(lx)
      break
    .end

    let ch = lexer_peek(lx)

    # spaces / tabs / carriage returns
    if ch == " " or ch == "\t" or ch == "\r"
      lexer_advance(lx)
      continue
    .end

    # newline: handled in lexer_next_token (optional keep)
    if ch == "\n"
      break
    .end

    # hash comments
    if lx.cfg.allow_hash_comments and ch == "#"
      lexer_skip_line_comment(lx)
      continue
    .end

    # // comments
    if lx.cfg.allow_slash_comments and ch == "/" and lexer_peek_n(lx, 1) == "/"
      lexer_skip_line_comment(lx)
      continue
    .end

    break
  .end
.end

fn lexer_skip_line_comment(mut lx: Lexer)
  while not lexer_is_eof(lx)
    let ch = lexer_peek(lx)
    if ch == "\n"
      break
    .end
    lexer_advance(lx)
  .end
.end

fn lexer_lex_ident_or_keyword(mut lx: Lexer) -> Token
  let startp = lexer_pos(lx)
  let start = lx.idx

  # first char
  lexer_advance(lx)
  let rest = lexer_take_while(lx, "ident_continue")

  let endi = lx.idx
  let lex = std_slice_str(lx.src, start, endi)
  let endp = lexer_pos(lx)
  let sp = span_new(startp, endp)

  let kind = keyword_or_ident(lex)
  ret token_new(kind, lex, sp)
.end

fn keyword_or_ident(s: str) -> TokenType
  if s == "let"      ret TokenType::KwLet .end
  if s == "const"    ret TokenType::KwConst .end
  if s == "fn"       ret TokenType::KwFn .end
  if s == "scn"      ret TokenType::KwScn .end
  if s == "ret"      ret TokenType::KwRet .end
  if s == "if"       ret TokenType::KwIf .end
  if s == "elif"     ret TokenType::KwElif .end
  if s == "else"     ret TokenType::KwElse .end
  if s == "while"    ret TokenType::KwWhile .end
  if s == "for"      ret TokenType::KwFor .end
  if s == "in"       ret TokenType::KwIn .end
  if s == "match"    ret TokenType::KwMatch .end
  if s == "break"    ret TokenType::KwBreak .end
  if s == "continue" ret TokenType::KwContinue .end
  if s == "true"     ret TokenType::KwTrue .end
  if s == "false"    ret TokenType::KwFalse .end
  if s == "module"   ret TokenType::KwModule .end
  if s == "use"      ret TokenType::KwUse .end
  if s == "type"     ret TokenType::KwType .end
  if s == "struct"   ret TokenType::KwStruct .end
  if s == "enum"     ret TokenType::KwEnum .end
  ret TokenType::Ident
.end

fn lexer_lex_number(mut lx: Lexer) -> Token
  let startp = lexer_pos(lx)
  let start = lx.idx

  # integer part
  lexer_take_while(lx, "digit")

  # fractional
  let is_float = false
  if lexer_peek(lx) == "." and is_digit(lexer_peek_n(lx, 1))
    is_float = true
    lexer_advance(lx) # '.'
    lexer_take_while(lx, "digit")
  .end

  # exponent (simple: e/E then optional +/- then digits)
  if lexer_peek(lx) == "e" or lexer_peek(lx) == "E"
    is_float = true
    lexer_advance(lx)
    if lexer_peek(lx) == "+" or lexer_peek(lx) == "-"
      lexer_advance(lx)
    .end
    lexer_take_while(lx, "digit")
  .end

  let endi = lx.idx
  let lex = std_slice_str(lx.src, start, endi)
  let endp = lexer_pos(lx)
  let sp = span_new(startp, endp)

  if is_float
    ret token_new(TokenType::Float, lex, sp)
  else
    ret token_new(TokenType::Int, lex, sp)
  .end
.end

fn lexer_lex_string(mut lx: Lexer) -> Token
  let startp = lexer_pos(lx)
  let start = lx.idx

  # opening quote
  lexer_advance(lx)

  let ok = true
  let esc = false
  while not lexer_is_eof(lx)
    let ch = lexer_peek(lx)

    if esc
      esc = false
      lexer_advance(lx)
      continue
    .end

    if ch == "\\"
      esc = true
      lexer_advance(lx)
      continue
    .end

    if ch == "\""
      lexer_advance(lx) # closing
      break
    .end

    if ch == "\n"
      ok = false
      break
    .end

    lexer_advance(lx)
  .end

  let endi = lx.idx
  let lex = std_slice_str(lx.src, start, endi)
  let endp = lexer_pos(lx)
  let sp = span_new(startp, endp)

  if not ok
    vec_push[Diag](lx.diags, diag_error("unterminated string literal", sp))
    ret token_new(TokenType::Error, lex, sp)
  .end

  ret token_new(TokenType::Str, lex, sp)
.end

fn lexer_lex_symbol(mut lx: Lexer) -> Token
  let startp = lexer_pos(lx)
  let ch = lexer_advance(lx)

  # two-char operators
  let next = lexer_peek(lx)

  if ch == "=" and next == "="
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::EqEq, "==", sp)
  .end

  if ch == "!" and next == "="
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::BangEq, "!=", sp)
  .end

  if ch == "<" and next == "="
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::Le, "<=", sp)
  .end

  if ch == ">" and next == "="
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::Ge, ">=", sp)
  .end

  if ch == "&" and next == "&"
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::AndAnd, "&&", sp)
  .end

  if ch == "|" and next == "|"
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::OrOr, "||", sp)
  .end

  if ch == "-" and next == ">"
    lexer_advance(lx)
    let sp = span_new(startp, lexer_pos(lx))
    ret token_new(TokenType::Arrow, "->", sp)
  .end

  # single-char
  let kind = symbol_kind(ch)
  let sp2 = span_new(startp, lexer_pos(lx))
  if kind == TokenType::Error
    vec_push[Diag](lx.diags, diag_error("unexpected character: " + ch, sp2))
  .end
  ret token_new(kind, ch, sp2)
.end

fn symbol_kind(ch: str) -> TokenType
  if ch == "=" ret TokenType::Eq .end
  if ch == "!" ret TokenType::Bang .end
  if ch == "+" ret TokenType::Plus .end
  if ch == "-" ret TokenType::Minus .end
  if ch == "*" ret TokenType::Star .end
  if ch == "/" ret TokenType::Slash .end
  if ch == "%" ret TokenType::Percent .end
  if ch == "<" ret TokenType::Lt .end
  if ch == ">" ret TokenType::Gt .end
  if ch == "&" ret TokenType::Amp .end
  if ch == "|" ret TokenType::Pipe .end
  if ch == "^" ret TokenType::Caret .end

  if ch == ":" ret TokenType::Colon .end
  if ch == ";" ret TokenType::Semi .end
  if ch == "," ret TokenType::Comma .end
  if ch == "." ret TokenType::Dot .end

  if ch == "(" ret TokenType::LParen .end
  if ch == ")" ret TokenType::RParen .end
  if ch == "[" ret TokenType::LBracket .end
  if ch == "]" ret TokenType::RBracket .end
  if ch == "{" ret TokenType::LBrace .end
  if ch == "}" ret TokenType::RBrace .end

  ret TokenType::Error
.end

# ============================================================================
# TokenBuffer (lookahead parsing)
# ============================================================================

type ParseError struct
  diag: Diag
.end

fn parse_error(msg: str, sp: Span) -> ParseError
  let e: ParseError
  e.diag = diag_error(msg, sp)
  ret e
.end

type TokenBuffer struct
  tokens: Vec[Token]
  cursor: int
.end

fn token_buffer_new(tokens: Vec[Token]) -> TokenBuffer
  let b: TokenBuffer
  b.tokens = tokens
  b.cursor = 0
  ret b
.end

fn token_buffer_pos(b: TokenBuffer) -> int
  ret b.cursor
.end

fn token_buffer_is_eof(b: TokenBuffer) -> bool
  match vec_get[Token](b.tokens, b.cursor)
    Option::Some(t) => ret t.kind == TokenType::Eof
    Option::None => ret true
  .end
.end

fn token_buffer_peek(b: TokenBuffer) -> Token
  match vec_get[Token](b.tokens, b.cursor)
    Option::Some(t) => ret t
    Option::None => ret token_new(TokenType::Eof, "", span_empty())
  .end
.end

fn token_buffer_peek_n(b: TokenBuffer, n: int) -> Token
  match vec_get[Token](b.tokens, b.cursor + n)
    Option::Some(t) => ret t
    Option::None => ret token_new(TokenType::Eof, "", span_empty())
  .end
.end

fn token_buffer_next(mut b: TokenBuffer) -> Token
  let t = token_buffer_peek(b)
  b.cursor = b.cursor + 1
  ret t
.end

fn token_buffer_consume_if(mut b: TokenBuffer, kind: TokenType) -> bool
  let t = token_buffer_peek(b)
  if t.kind == kind
    b.cursor = b.cursor + 1
    ret true
  .end
  ret false
.end

fn token_buffer_expect(mut b: TokenBuffer, kind: TokenType) -> Result[Token, ParseError]
  let t = token_buffer_peek(b)
  if t.kind == kind
    b.cursor = b.cursor + 1
    ret Result::Ok(t)
  .end
  ret Result::Err(parse_error("expected token: " + token_type_name(kind), t.span))
.end

fn token_type_name(k: TokenType) -> str
  match k
    TokenType::Eof => ret "Eof"
    TokenType::Error => ret "Error"
    TokenType::Newline => ret "Newline"

    TokenType::Ident => ret "Ident"
    TokenType::Int => ret "Int"
    TokenType::Float => ret "Float"
    TokenType::Str => ret "Str"

    TokenType::KwLet => ret "let"
    TokenType::KwConst => ret "const"
    TokenType::KwFn => ret "fn"
    TokenType::KwScn => ret "scn"
    TokenType::KwRet => ret "ret"
    TokenType::KwIf => ret "if"
    TokenType::KwElif => ret "elif"
    TokenType::KwElse => ret "else"
    TokenType::KwWhile => ret "while"
    TokenType::KwFor => ret "for"
    TokenType::KwIn => ret "in"
    TokenType::KwMatch => ret "match"
    TokenType::KwBreak => ret "break"
    TokenType::KwContinue => ret "continue"
    TokenType::KwTrue => ret "true"
    TokenType::KwFalse => ret "false"
    TokenType::KwModule => ret "module"
    TokenType::KwUse => ret "use"
    TokenType::KwType => ret "type"
    TokenType::KwStruct => ret "struct"
    TokenType::KwEnum => ret "enum"

    TokenType::Eq => ret "="
    TokenType::EqEq => ret "=="
    TokenType::Bang => ret "!"
    TokenType::BangEq => ret "!="
    TokenType::Plus => ret "+"
    TokenType::Minus => ret "-"
    TokenType::Star => ret "*"
    TokenType::Slash => ret "/"
    TokenType::Percent => ret "%"
    TokenType::Lt => ret "<"
    TokenType::Le => ret "<="
    TokenType::Gt => ret ">"
    TokenType::Ge => ret ">="
    TokenType::AndAnd => ret "&&"
    TokenType::OrOr => ret "||"
    TokenType::Amp => ret "&"
    TokenType::Pipe => ret "|"
    TokenType::Caret => ret "^"

    TokenType::Colon => ret ":"
    TokenType::Semi => ret ";"
    TokenType::Comma => ret ","
    TokenType::Dot => ret "."
    TokenType::Arrow => ret "->"

    TokenType::LParen => ret "("
    TokenType::RParen => ret ")"
    TokenType::LBracket => ret "["
    TokenType::RBracket => ret "]"
    TokenType::LBrace => ret "{" 
    TokenType::RBrace => ret "}"
  .end
.end

# ============================================================================
# Bytecode representation
# ============================================================================

# Operand indexes types

type LocalId = int

type ConstId = int

type LabelId = int

# Bytecode operations (IR-level)

type BytecodeOp enum
  Nop

  # constants
  ConstI64(value: i64)
  ConstF64(value: f64)
  ConstStr(value: str)

  # locals
  LoadLocal(id: LocalId)
  StoreLocal(id: LocalId)

  # stack ops
  Pop
  Dup

  # arithmetic
  Add
  Sub
  Mul
  Div
  Rem
  Neg

  # compare
  Eq
  Ne
  Lt
  Le
  Gt
  Ge

  # control flow
  Jump(label: LabelId)
  JumpIfFalse(label: LabelId)
  Call(argc: int)
  Return
  Halt
.end

# Instruction with span

type Instr struct
  op: BytecodeOp
  span: Span
.end

fn instr_new(op: BytecodeOp, sp: Span) -> Instr
  let i: Instr
  i.op = op
  i.span = sp
  ret i
.end

type Bytecode struct
  code: Vec[Instr]
.end

fn bytecode_new() -> Bytecode
  let b: Bytecode
  b.code = vec_new[Instr]()
  ret b
.end

fn bytecode_len(b: Bytecode) -> int
  ret vec_len[Instr](b.code)
.end

fn bytecode_emit(mut b: Bytecode, op: BytecodeOp)
  vec_push[Instr](b.code, instr_new(op, span_empty()))
.end

fn bytecode_emit_spanned(mut b: Bytecode, op: BytecodeOp, sp: Span)
  vec_push[Instr](b.code, instr_new(op, sp))
.end

fn bytecode_get(b: Bytecode, ip: int) -> Option[Instr]
  ret vec_get[Instr](b.code, ip)
.end

fn bytecode_disasm(b: Bytecode) -> str
  let out = ""
  let n = vec_len[Instr](b.code)
  let i = 0
  while i < n
    match vec_get[Instr](b.code, i)
      Option::Some(ins) =>
        out = out + std_i64_to_str(std_i64_from_int(i)) + ": " + bytecode_op_to_string(ins.op) + "\n"
      Option::None => out = out
    .end
    i = i + 1
  .end
  ret out
.end

fn bytecode_op_to_string(op: BytecodeOp) -> str
  match op
    BytecodeOp::Nop => ret "NOP"

    BytecodeOp::ConstI64(v) => ret "CONST_I64 " + std_i64_to_str(v)
    BytecodeOp::ConstF64(v) => ret "CONST_F64 " + std_f64_to_str(v)
    BytecodeOp::ConstStr(s) => ret "CONST_STR \"" + s + "\""

    BytecodeOp::LoadLocal(id) => ret "LOAD_LOCAL " + std_i64_to_str(std_i64_from_int(id))
    BytecodeOp::StoreLocal(id) => ret "STORE_LOCAL " + std_i64_to_str(std_i64_from_int(id))

    BytecodeOp::Pop => ret "POP"
    BytecodeOp::Dup => ret "DUP"

    BytecodeOp::Add => ret "ADD"
    BytecodeOp::Sub => ret "SUB"
    BytecodeOp::Mul => ret "MUL"
    BytecodeOp::Div => ret "DIV"
    BytecodeOp::Rem => ret "REM"
    BytecodeOp::Neg => ret "NEG"

    BytecodeOp::Eq => ret "EQ"
    BytecodeOp::Ne => ret "NE"
    BytecodeOp::Lt => ret "LT"
    BytecodeOp::Le => ret "LE"
    BytecodeOp::Gt => ret "GT"
    BytecodeOp::Ge => ret "GE"

    BytecodeOp::Jump(l) => ret "JUMP L" + std_i64_to_str(std_i64_from_int(l))
    BytecodeOp::JumpIfFalse(l) => ret "JIF_FALSE L" + std_i64_to_str(std_i64_from_int(l))
    BytecodeOp::Call(argc) => ret "CALL " + std_i64_to_str(std_i64_from_int(argc))
    BytecodeOp::Return => ret "RET"
    BytecodeOp::Halt => ret "HALT"
  .end
.end

# ============================================================================
# Predicates / char helpers
# ============================================================================

fn is_digit(ch: str) -> bool
  if ch == "" ret false .end
  let c = std_char_code(ch)
  ret c >= std_char_code("0") and c <= std_char_code("9")
.end

fn is_hex_digit(ch: str) -> bool
  if ch == "" ret false .end
  let c = std_char_code(ch)
  if c >= std_char_code("0") and c <= std_char_code("9")
    ret true
  .end
  if c >= std_char_code("a") and c <= std_char_code("f")
    ret true
  .end
  if c >= std_char_code("A") and c <= std_char_code("F")
    ret true
  .end
  ret false
.end

fn is_ident_start(ch: str) -> bool
  if ch == "" ret false .end
  let c = std_char_code(ch)
  if c == std_char_code("_")
    ret true
  .end
  if c >= std_char_code("a") and c <= std_char_code("z")
    ret true
  .end
  if c >= std_char_code("A") and c <= std_char_code("Z")
    ret true
  .end
  ret false
.end

fn is_ident_continue(ch: str) -> bool
  if is_ident_start(ch)
    ret true
  .end
  if is_digit(ch)
    ret true
  .end
  ret false
.end

# ============================================================================
# Smoke tests
# ============================================================================

fn _assert(cond: bool, msg: str)
  if not cond
    panic(msg)
  .end
.end

fn test_lexer_smoke()
  let lx = lexer_new("x = 42\nlet y = \"ok\"" )
  let toks = lexer_tokenize(lx)
  _assert(vec_len[Token](toks) >= 2, "expected tokens")

  # first should be ident
  match vec_get[Token](toks, 0)
    Option::Some(t0) => _assert(t0.kind == TokenType::Ident, "t0 kind")
    Option::None => panic("missing t0")
  .end
.end

fn test_bytecode_smoke()
  let bc = bytecode_new()
  bytecode_emit(bc, BytecodeOp::ConstI64(42))
  bytecode_emit(bc, BytecodeOp::Return)
  _assert(bytecode_len(bc) == 2, "bytecode len")
.end

fn vitte_beryl_smoke()
  test_lexer_smoke()
  test_bytecode_smoke()
.end

.end

# ============================================================================
# std_* stubs (wire to your real std)
# ============================================================================

# Arrays
fn std_len[T](xs: [T]) -> int
  ret __std_len(xs)
.end

# Strings
fn std_len_str(s: str) -> int
  ret __std_len_str(s)
.end

fn std_char_at_str(s: str, idx: int) -> str
  ret __std_char_at_str(s, idx)
.end

fn std_slice_str(s: str, lo: int, hi: int) -> str
  ret __std_slice_str(s, lo, hi)
.end

# Char code (single-char string -> int)
fn std_char_code(ch: str) -> int
  ret __std_char_code(ch)
.end

# Formatting helpers
fn std_i64_to_str(v: i64) -> str
  ret __std_i64_to_str(v)
.end

fn std_f64_to_str(v: f64) -> str
  ret __std_f64_to_str(v)
.end

fn std_i64_from_int(v: int) -> i64
  ret __std_i64_from_int(v)
.end
