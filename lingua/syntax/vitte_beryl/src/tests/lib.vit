

# vitte_beryl.tests
#
# Tests unit + smoke pour vitte_beryl (lexer/token buffer/bytecode).
#
# Contrainte: pas dâ€™accolades, blocs `.end`.

module vitte_beryl.tests

use vitte_beryl.impls

# ---------------------------------------------------------------------------
# Mini assert helpers
# ---------------------------------------------------------------------------

fn _panic(msg: str)
  panic(msg)
.end

fn assert_true(cond: bool, msg: str)
  if not cond
    _panic(msg)
  .end
.end

fn assert_eq_str(a: str, b: str, msg: str)
  if a != b
    _panic(msg + " (expected='" + b + "' got='" + a + "')")
  .end
.end

fn assert_eq_int(a: int, b: int, msg: str)
  if a != b
    _panic(msg)
  .end
.end

fn assert_eq_tok_kind(t: Token, k: TokenType, msg: str)
  if t.kind != k
    _panic(msg + " (expected=" + token_type_name(k) + " got=" + token_type_name(t.kind) + ")")
  .end
.end

# Extract token safely
fn tok_at(ts: Vec[Token], i: int) -> Token
  match vec_get[Token](ts, i)
    Option::Some(t) => ret t
    Option::None =>
      _panic("missing token at index " + std_i64_to_str(std_i64_from_int(i)))
      ret token_new(TokenType::Eof, "", span_empty())
  .end
.end

# ---------------------------------------------------------------------------
# Lexer tests
# ---------------------------------------------------------------------------

fn test_lex_basic_assignment()
  let lx = lexer_new("x = 42")
  let toks = lexer_tokenize(lx)

  # x, =, 42, eof
  assert_eq_tok_kind(tok_at(toks, 0), TokenType::Ident, "t0 ident")
  assert_eq_str(tok_at(toks, 0).lexeme, "x", "ident lexeme")

  assert_eq_tok_kind(tok_at(toks, 1), TokenType::Eq, "t1 eq")

  let t2 = tok_at(toks, 2)
  assert_eq_tok_kind(t2, TokenType::Int, "t2 int")
  assert_eq_str(t2.lexeme, "42", "int lexeme")

  assert_eq_tok_kind(tok_at(toks, 3), TokenType::Eof, "t3 eof")
.end

fn test_lex_keywords_and_idents()
  let lx = lexer_new("let const fn scn ret if elif else while for in match break continue true false module use type struct enum ident")
  let toks = lexer_tokenize(lx)

  assert_eq_tok_kind(tok_at(toks, 0), TokenType::KwLet, "kw let")
  assert_eq_tok_kind(tok_at(toks, 1), TokenType::KwConst, "kw const")
  assert_eq_tok_kind(tok_at(toks, 2), TokenType::KwFn, "kw fn")
  assert_eq_tok_kind(tok_at(toks, 3), TokenType::KwScn, "kw scn")
  assert_eq_tok_kind(tok_at(toks, 4), TokenType::KwRet, "kw ret")
  assert_eq_tok_kind(tok_at(toks, 5), TokenType::KwIf, "kw if")
  assert_eq_tok_kind(tok_at(toks, 6), TokenType::KwElif, "kw elif")
  assert_eq_tok_kind(tok_at(toks, 7), TokenType::KwElse, "kw else")
  assert_eq_tok_kind(tok_at(toks, 8), TokenType::KwWhile, "kw while")
  assert_eq_tok_kind(tok_at(toks, 9), TokenType::KwFor, "kw for")
  assert_eq_tok_kind(tok_at(toks, 10), TokenType::KwIn, "kw in")
  assert_eq_tok_kind(tok_at(toks, 11), TokenType::KwMatch, "kw match")
  assert_eq_tok_kind(tok_at(toks, 12), TokenType::KwBreak, "kw break")
  assert_eq_tok_kind(tok_at(toks, 13), TokenType::KwContinue, "kw continue")
  assert_eq_tok_kind(tok_at(toks, 14), TokenType::KwTrue, "kw true")
  assert_eq_tok_kind(tok_at(toks, 15), TokenType::KwFalse, "kw false")
  assert_eq_tok_kind(tok_at(toks, 16), TokenType::KwModule, "kw module")
  assert_eq_tok_kind(tok_at(toks, 17), TokenType::KwUse, "kw use")
  assert_eq_tok_kind(tok_at(toks, 18), TokenType::KwType, "kw type")
  assert_eq_tok_kind(tok_at(toks, 19), TokenType::KwStruct, "kw struct")
  assert_eq_tok_kind(tok_at(toks, 20), TokenType::KwEnum, "kw enum")

  assert_eq_tok_kind(tok_at(toks, 21), TokenType::Ident, "ident")
  assert_eq_str(tok_at(toks, 21).lexeme, "ident", "ident lexeme")
.end

fn test_lex_operators()
  let lx = lexer_new("== != <= >= && || -> = ! < > + - * / % & | ^ : ; , . ( ) [ ] { }")
  let toks = lexer_tokenize(lx)

  assert_eq_tok_kind(tok_at(toks, 0), TokenType::EqEq, "==")
  assert_eq_tok_kind(tok_at(toks, 1), TokenType::BangEq, "!=")
  assert_eq_tok_kind(tok_at(toks, 2), TokenType::Le, "<=")
  assert_eq_tok_kind(tok_at(toks, 3), TokenType::Ge, ">=")
  assert_eq_tok_kind(tok_at(toks, 4), TokenType::AndAnd, "&&")
  assert_eq_tok_kind(tok_at(toks, 5), TokenType::OrOr, "||")
  assert_eq_tok_kind(tok_at(toks, 6), TokenType::Arrow, "->")

  assert_eq_tok_kind(tok_at(toks, 7), TokenType::Eq, "=")
  assert_eq_tok_kind(tok_at(toks, 8), TokenType::Bang, "!")
  assert_eq_tok_kind(tok_at(toks, 9), TokenType::Lt, "<")
  assert_eq_tok_kind(tok_at(toks, 10), TokenType::Gt, ">")

  assert_eq_tok_kind(tok_at(toks, 11), TokenType::Plus, "+")
  assert_eq_tok_kind(tok_at(toks, 12), TokenType::Minus, "-")
  assert_eq_tok_kind(tok_at(toks, 13), TokenType::Star, "*")
  assert_eq_tok_kind(tok_at(toks, 14), TokenType::Slash, "/")
  assert_eq_tok_kind(tok_at(toks, 15), TokenType::Percent, "%")

  assert_eq_tok_kind(tok_at(toks, 16), TokenType::Amp, "&")
  assert_eq_tok_kind(tok_at(toks, 17), TokenType::Pipe, "|")
  assert_eq_tok_kind(tok_at(toks, 18), TokenType::Caret, "^")

  assert_eq_tok_kind(tok_at(toks, 19), TokenType::Colon, ":")
  assert_eq_tok_kind(tok_at(toks, 20), TokenType::Semi, ";")
  assert_eq_tok_kind(tok_at(toks, 21), TokenType::Comma, ",")
  assert_eq_tok_kind(tok_at(toks, 22), TokenType::Dot, ".")

  assert_eq_tok_kind(tok_at(toks, 23), TokenType::LParen, "(")
  assert_eq_tok_kind(tok_at(toks, 24), TokenType::RParen, ")")
  assert_eq_tok_kind(tok_at(toks, 25), TokenType::LBracket, "[")
  assert_eq_tok_kind(tok_at(toks, 26), TokenType::RBracket, "]")
  assert_eq_tok_kind(tok_at(toks, 27), TokenType::LBrace, "{")
  assert_eq_tok_kind(tok_at(toks, 28), TokenType::RBrace, "}")
.end

fn test_lex_numbers_int_float()
  let lx = lexer_new("0 1 42 3.14 10e3 10E-3 1.2e+4")
  let toks = lexer_tokenize(lx)

  assert_eq_tok_kind(tok_at(toks, 0), TokenType::Int, "0 int")
  assert_eq_tok_kind(tok_at(toks, 1), TokenType::Int, "1 int")
  assert_eq_tok_kind(tok_at(toks, 2), TokenType::Int, "42 int")

  assert_eq_tok_kind(tok_at(toks, 3), TokenType::Float, "3.14 float")
  assert_eq_tok_kind(tok_at(toks, 4), TokenType::Float, "10e3 float")
  assert_eq_tok_kind(tok_at(toks, 5), TokenType::Float, "10E-3 float")
  assert_eq_tok_kind(tok_at(toks, 6), TokenType::Float, "1.2e+4 float")
.end

fn test_lex_strings_and_escapes()
  let lx = lexer_new("\"a\" \"b\\\"c\" \"x\\n y\"")
  let toks = lexer_tokenize(lx)

  assert_eq_tok_kind(tok_at(toks, 0), TokenType::Str, "str0")
  assert_eq_str(tok_at(toks, 0).lexeme, "\"a\"", "str0 lexeme")

  assert_eq_tok_kind(tok_at(toks, 1), TokenType::Str, "str1")
  assert_eq_str(tok_at(toks, 1).lexeme, "\"b\\\"c\"", "str1 lexeme")

  assert_eq_tok_kind(tok_at(toks, 2), TokenType::Str, "str2")
.end

fn test_lex_comments_hash_and_slashslash()
  let src = "x # comment\n y // c2\n z"
  let cfg = lexer_config_default()
  cfg.keep_newlines = false
  cfg.allow_hash_comments = true
  cfg.allow_slash_comments = true

  let lx = lexer_with_config(src, cfg)
  let toks = lexer_tokenize(lx)

  # Expect: x, y, z, eof
  assert_eq_str(tok_at(toks, 0).lexeme, "x", "x")
  assert_eq_str(tok_at(toks, 1).lexeme, "y", "y")
  assert_eq_str(tok_at(toks, 2).lexeme, "z", "z")
  assert_eq_tok_kind(tok_at(toks, 3), TokenType::Eof, "eof")
.end

fn test_lex_keep_newlines()
  let src = "x\n\n y"
  let cfg = lexer_config_default()
  cfg.keep_newlines = true

  let lx = lexer_with_config(src, cfg)
  let toks = lexer_tokenize(lx)

  # x, NL, NL, y, eof
  assert_eq_tok_kind(tok_at(toks, 0), TokenType::Ident, "x")
  assert_eq_tok_kind(tok_at(toks, 1), TokenType::Newline, "nl1")
  assert_eq_tok_kind(tok_at(toks, 2), TokenType::Newline, "nl2")
  assert_eq_tok_kind(tok_at(toks, 3), TokenType::Ident, "y")
  assert_eq_tok_kind(tok_at(toks, 4), TokenType::Eof, "eof")
.end

fn test_lex_positions_line_col()
  let src = "a b\n  c"
  let cfg = lexer_config_default()
  cfg.keep_newlines = true
  let lx = lexer_with_config(src, cfg)
  let toks = lexer_tokenize(lx)

  let t0 = tok_at(toks, 0)
  let t1 = tok_at(toks, 1)
  let t2 = tok_at(toks, 2) # newline
  let t3 = tok_at(toks, 3) # c

  assert_eq_int(t0.span.start.line, 1, "t0 line")
  assert_eq_int(t0.span.start.col, 1, "t0 col")

  assert_eq_int(t1.span.start.line, 1, "t1 line")
  assert_true(t1.span.start.col >= 3, "t1 col >= 3")

  assert_eq_int(t2.kind == TokenType::Newline, true, "t2 newline")

  assert_eq_int(t3.span.start.line, 2, "t3 line")
  assert_true(t3.span.start.col >= 3, "t3 col >= 3")
.end

fn test_lex_unterminated_string_emits_error_token()
  let lx = lexer_new("\"unterminated\nnext")
  let toks = lexer_tokenize(lx)

  let t0 = tok_at(toks, 0)
  assert_eq_tok_kind(t0, TokenType::Error, "unterminated string => Error token")
.end

# ---------------------------------------------------------------------------
# TokenBuffer tests
# ---------------------------------------------------------------------------

fn test_token_buffer_peek_next_expect()
  let lx = lexer_new("x = 1")
  let toks = lexer_tokenize(lx)
  let b = token_buffer_new(toks)

  let p0 = token_buffer_peek(b)
  assert_eq_tok_kind(p0, TokenType::Ident, "peek ident")

  let t0 = token_buffer_next(b)
  assert_eq_tok_kind(t0, TokenType::Ident, "next ident")

  let r = token_buffer_expect(b, TokenType::Eq)
  match r
    Result::Ok(t1) => assert_eq_tok_kind(t1, TokenType::Eq, "expect eq")
    Result::Err(_) => _panic("expect eq failed")
  .end

  let ok1 = token_buffer_consume_if(b, TokenType::Int)
  assert_true(ok1, "consume int")

  assert_true(token_buffer_is_eof(b), "buffer eof")
.end

fn test_token_buffer_expect_error()
  let lx = lexer_new("x")
  let toks = lexer_tokenize(lx)
  let b = token_buffer_new(toks)

  # consume ident
  token_buffer_next(b)

  let r = token_buffer_expect(b, TokenType::Eq)
  match r
    Result::Ok(_) => _panic("expected error")
    Result::Err(e) => assert_true(e.diag.level == DiagLevel::Error, "diag level")
  .end
.end

# ---------------------------------------------------------------------------
# Bytecode tests
# ---------------------------------------------------------------------------

fn test_bytecode_emit_and_disasm()
  let bc = bytecode_new()
  bytecode_emit(bc, BytecodeOp::ConstI64(123))
  bytecode_emit(bc, BytecodeOp::LoadLocal(0))
  bytecode_emit(bc, BytecodeOp::Add)
  bytecode_emit(bc, BytecodeOp::Return)

  assert_eq_int(bytecode_len(bc), 4, "bytecode len")

  let s = bytecode_disasm(bc)
  assert_true(std_contains(s, "CONST_I64"), "disasm contains CONST_I64")
  assert_true(std_contains(s, "RET"), "disasm contains RET")
.end

# ---------------------------------------------------------------------------
# Master runner
# ---------------------------------------------------------------------------

fn vitte_beryl_tests_all()
  # lexer
  test_lex_basic_assignment()
  test_lex_keywords_and_idents()
  test_lex_operators()
  test_lex_numbers_int_float()
  test_lex_strings_and_escapes()
  test_lex_comments_hash_and_slashslash()
  test_lex_keep_newlines()
  test_lex_positions_line_col()
  test_lex_unterminated_string_emits_error_token()

  # token buffer
  test_token_buffer_peek_next_expect()
  test_token_buffer_expect_error()

  # bytecode
  test_bytecode_emit_and_disasm()

  # smoke (impls)
  vitte_beryl.impls.vitte_beryl_smoke()
.end

.end

# ---------------------------------------------------------------------------
# std helper (tests only)
# ---------------------------------------------------------------------------

fn std_contains(hay: str, needle: str) -> bool
  ret __std_contains(hay, needle)
.end